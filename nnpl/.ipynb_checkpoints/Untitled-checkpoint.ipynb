{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Blob:\n",
    "    def __init__(self):\n",
    "        self.data_ = None\n",
    "        self.diff_ = None\n",
    "        self.is_update_ = True\n",
    "        return\n",
    "    \n",
    "    def setup(self, shape):\n",
    "        num_args = len(shape)\n",
    "        assert num_args == 1 or num_args == 2, 'the number of arguments should either be 1 or 2'\n",
    "        self.data_ = np.empty(shape, dtype=float)  # of size (natoms#1+natoms#2+...) * feature_size\n",
    "        self.diff_ = np.empty(shape, dtype=float) # of same size of self.coors\n",
    "        return\n",
    "    \n",
    "    def shape(self):\n",
    "        assert self.data_.shape == self.diff_.shape, 'the shape of data_ does not match with the shape of diff_'\n",
    "        return self.data_.shape\n",
    "    \n",
    "    def update(self):\n",
    "        if self.is_update_:\n",
    "#             print 'updating'\n",
    "#             print 'before: ', self.data_[0, 0]\n",
    "            assert self.data_.shape == self.diff_.shape, 'the shape of data_ does not match with the shape of diff_'\n",
    "            self.data_ -= self.diff_\n",
    "#             print 'after: ', self.data_[0, 0]\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Filler:\n",
    "    def __init__(self, param):\n",
    "        #         self.param_ = copy.deepcopy(param)\n",
    "        self.type_ = param['type']\n",
    "        self.param_ = copy.deepcopy(param)\n",
    "        return\n",
    "    \n",
    "    def fill(self, shape):\n",
    "        #         shape = container.shape\n",
    "        size =  reduce(lambda count, item: count * item, shape, 1)\n",
    "        weight = Blob()\n",
    "        if self.type_ == 'gaussian':\n",
    "            std = self.param_.get('std', 0.01)\n",
    "            weight.data_ = np.random.randn(size).reshape(shape) * std\n",
    "        elif self.type_ == 'constant':\n",
    "            value = self.param_.get('value', 0.0)\n",
    "            weight.data_ = np.ones(size).reshape(shape) * value\n",
    "        return weight\n",
    "    \n",
    "# param = {'type': 'gaussian', 'std': 0.1}\n",
    "# filler = Filler(param)\n",
    "# w = filler.fill(np.array([20, 30]))\n",
    "# plt.hist(w.reshape((-1)), bins=20)\n",
    "\n",
    "\n",
    "# param = {'type': 'constant'}\n",
    "# filler = Filler(param)\n",
    "# w = filler.fill(np.array([20]))\n",
    "# w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_creator = {}\n",
    "def layer_register(layer_name, layer):\n",
    "    assert layer_name not in layer_creator.keys(), 'layer %s has already been defined'%layer_name\n",
    "    layer_creator[layer_name] = layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TxtReader:\n",
    "    '''\n",
    "    very inefficent\n",
    "    '''\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path_ = file_path\n",
    "        self.multiple_label_ = None\n",
    "        with open(self.file_path_, 'r') as fp:\n",
    "            line = fp.readline()\n",
    "        if len(line.split(',')) == 1:\n",
    "            self.multiple_label_=False\n",
    "        else:\n",
    "            self.multiple_label_=True\n",
    "        return\n",
    "    \n",
    "    def fetch_data(self, line_num):\n",
    "        with open(self.file_path_, 'r') as fp:\n",
    "            line = fp.readlines()[line_num]\n",
    "        if self.multiple_label_:\n",
    "            result = np.array(map(float, line.split(',')))\n",
    "        else:\n",
    "            result = np.array(float(line.strip()))\n",
    "        return result\n",
    "        \n",
    "    def fetch_data(self, start_line, end_line):\n",
    "        with open(self.file_path_, 'r') as fp:\n",
    "            lines = fp.readlines()[start_line: end_line]\n",
    "        if self.multiple_label_:\n",
    "            result = np.array([map(float, line.split(',')) for line in lines])\n",
    "        else:\n",
    "            result = np.array([float(line.strip()) for line in lines])\n",
    "        return result\n",
    "        return np.array([map(float, line.split(',')) for line in lines])\n",
    "        \n",
    "    def peek(self):\n",
    "        with open(self.file_path_, 'r') as fp:\n",
    "            line = fp.readline()\n",
    "        if self.multiple_label_:\n",
    "            result = np.array(map(float, line.split(',')))\n",
    "        else:\n",
    "            result = np.array(float(line.strip()))\n",
    "        return result\n",
    "    \n",
    "# a = TxtReader('./iris/labels.dat')\n",
    "# print a.multiple_label_\n",
    "# a.fetch_data(0, 2).shape\n",
    "\n",
    "if 'TxtData' in layer_creator:\n",
    "    del layer_creator['TxtData']\n",
    "\n",
    "class TxtDataLayer:\n",
    "    '''\n",
    "    TODO: support none value in feature file, support shuffle\n",
    "    data layer for inputs from text format\n",
    "    expecting two txt files each for features and labels\n",
    "    one line for one entry\n",
    "    features should be seperated by commas\n",
    "    only support single label for each entry\n",
    "    '''\n",
    "    def __init__(self, param):\n",
    "        '''{'feature_file': 'path/to/feature/file', 'label_file': 'path/to/label/file', 'batch_size': batch_size, 'num_entries': num_entries}'''\n",
    "        self.blobs_ = None\n",
    "        self.batch_size_ = param['batch_size']\n",
    "        self.feature_reader_ = TxtReader(param['feature_file'])\n",
    "        self.label_reader_ = TxtReader(param['label_file'])\n",
    "        \n",
    "        self.idx_ = 0 # the current file number\n",
    "        self.num_entries_ = param['num_entries'] # the number of entries in the feature file\n",
    "        return\n",
    "    \n",
    "    def setup(self, bottoms, tops):\n",
    "        '''\n",
    "        tops[0] for feature blob, tops[1] for label blob\n",
    "        '''\n",
    "        assert len(tops) == 2, 'layer TxtDataLayer only support bottoms of length 2'\n",
    "        num_features = self.feature_reader_.peek().shape[0]\n",
    "        tops[0].setup((self.batch_size_, num_features))\n",
    "        tops[1].setup((self.batch_size_, ))\n",
    "        tops[0].is_update_ = False\n",
    "        tops[1].is_update_ = False\n",
    "        return\n",
    "    \n",
    "    def forward(self, bottoms, tops):\n",
    "        # store data in text file is highly inefficent, would better to use database like lmdb or leveldb\n",
    "        # take care of when idx_+batch_size_ larger than num_entries\n",
    "        loops = []\n",
    "        num_loops = (self.idx_ + self.batch_size_) / self.num_entries_\n",
    "        if num_loops == 0:\n",
    "            loops.append((self.idx_, self.idx_+self.batch_size_))\n",
    "        else:\n",
    "            loops.append((self.idx_, self.num_entries_))\n",
    "            for i in range(1, num_loops):\n",
    "                loops.append((0, self.num_entries_))\n",
    "            if (self.idx_ + self.batch_size_) % self.num_entries_ != 0:\n",
    "                loops.append((0, (self.idx_ + self.batch_size_) % self.num_entries_))\n",
    "        self.idx_ = (self.idx_ + self.batch_size_) % self.num_entries_\n",
    "        \n",
    "        batch_idx = 0\n",
    "        for loop in loops:\n",
    "#             print loop\n",
    "#             print tops[0].data_[batch_idx: batch_idx + loop[1] - loop[0]].shape\n",
    "#             print self.feature_reader_.fetch_data(loop[0], loop[1]).shape\n",
    "            tops[0].data_[batch_idx: batch_idx + loop[1] - loop[0]] = self.feature_reader_.fetch_data(loop[0], loop[1])\n",
    "            tops[1].data_[batch_idx: batch_idx + loop[1] - loop[0]] = self.label_reader_.fetch_data(loop[0], loop[1])\n",
    "            batch_idx = batch_idx + loop[1] - loop[0]\n",
    "        return\n",
    "    \n",
    "    def backward(self, bottoms, tops):\n",
    "        return\n",
    "\n",
    "layer_register('TxtData', TxtDataLayer)\n",
    "# txt_data_param = {'batch_size': 50, 'feature_file': './iris/features.dat', 'label_file': './iris/labels.dat', 'num_entries': 150}\n",
    "# tdl = TxtDataLayer(txt_data_param)\n",
    "\n",
    "# fb, lb = Blob(), Blob()\n",
    "# tops = [fb, lb]\n",
    "# bottoms = None\n",
    "\n",
    "# tdl.setup(bottoms, tops)\n",
    "# tdl.forward(bottoms, tops)\n",
    "# print tops[0].data_[:2]\n",
    "# print tops[1].data_[:2]\n",
    "# tdl.forward(bottoms, tops)\n",
    "# print tops[0].data_[:2]\n",
    "# print tops[1].data_[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'InnerProduct' in layer_creator:\n",
    "    del layer_creator['InnerProduct']\n",
    "\n",
    "class InnerProductLayer:\n",
    "    \n",
    "    def __init__(self, param):\n",
    "        self.output_size_ = param['output_size']\n",
    "        self.weight_filler_param_ = copy.deepcopy(param['weight_filler_param'])\n",
    "        self.bias_filler_param_ = copy.deepcopy(param['bias_filler_param'])\n",
    "        self.input_size_ = None\n",
    "        self.batch_size_ = None\n",
    "#         self.weight_ = None\n",
    "#         self.bias_ = None\n",
    "        self.blobs_ = [None, None]  # [weight_, bias_]\n",
    "        return\n",
    "        \n",
    "    def setup(self, bottoms, tops):\n",
    "        assert len(bottoms) == 1, 'layer InnerProductLayer only support bottoms of length 1'\n",
    "        assert len(tops) == 1, 'layer InnerProductLayer only support tops of length 1'\n",
    "        data = bottoms[0].data_\n",
    "        self.batch_size_, self.input_size_ = data.shape\n",
    "        tops[0].setup((self.batch_size_, self.output_size_))\n",
    "        weight_filler = Filler(self.weight_filler_param_)\n",
    "        self.blobs_[0] = weight_filler.fill((self.input_size_, self.output_size_))\n",
    "        bias_filler = Filler(self.bias_filler_param_)\n",
    "        self.blobs_[1] = bias_filler.fill((self.output_size_, ))\n",
    "        return\n",
    "    \n",
    "    def forward(self, bottoms, tops):\n",
    "        tops[0].data_ = np.dot(bottoms[0].data_, self.blobs_[0].data_) + self.blobs_[1].data_\n",
    "        return\n",
    "        \n",
    "    def backward(self, bottoms, tops):\n",
    "        bottoms[0].diff_ = np.dot(self.blobs_[0].data_, tops[0].diff_.T).T\n",
    "        self.blobs_[0].diff_ = np.dot(bottoms[0].data_.T, tops[0].diff_)\n",
    "        self.blobs_[1].diff_ = np.sum(tops[0].diff_, axis=0)\n",
    "        return\n",
    "\n",
    "layer_register('InnerProduct', InnerProductLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'SoftmaxLoss' in layer_creator:\n",
    "    del layer_creator['SoftmaxLoss']\n",
    "class SoftmaxLossLayer:\n",
    "    '''\n",
    "    bottoms[0] of shape (batch_size * num_labels), unnormalized exp prob\n",
    "    bottoms[1] of shape(batch_size * 1), ground truth label for each batch\n",
    "    entries bottom[1] should be in range(0, num_labels)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, param):\n",
    "        self.prob_ = None\n",
    "        self.blobs_ = None\n",
    "        return\n",
    "    \n",
    "    def setup(self, bottoms, tops):\n",
    "        assert len(bottoms) == 2, 'layer SoftmaxLossLayer only support bottoms of length 1'\n",
    "        return\n",
    "    \n",
    "    def forward(self, bottoms, tops):\n",
    "        self.prob_ = np.exp(bottoms[0].data_ - np.max(bottoms[0].data_, axis=1, keepdims=True))\n",
    "        self.prob_ /= np.sum(self.prob_, axis=1, keepdims=True)\n",
    "        N = bottoms[0].data_.shape[0]\n",
    "        loss = -np.sum(np.log(self.prob_[np.arange(N), list(bottoms[1].data_)])) / N\n",
    "        print loss\n",
    "        return\n",
    "    \n",
    "    def backward(self, bottoms, tops):\n",
    "        bottoms[0].diff_ = self.prob_.copy()\n",
    "        N = bottoms[0].data_.shape[0]\n",
    "        bottoms[0].diff_[np.arange(N), list(bottoms[1].data_)] -= 1\n",
    "        bottoms[0].diff_ /= N\n",
    "        return\n",
    "\n",
    "layer_register('SoftmaxLoss', SoftmaxLossLayer)\n",
    "\n",
    "# bottoms = [None] * 2\n",
    "# bottoms[0] = Blob()\n",
    "# bottoms[0].data_ = np.random.randn(20, 40)\n",
    "# bottoms[1] = Blob()\n",
    "# bottoms[1].data_ = np.random.randint(40, size=20)\n",
    "# tops = []\n",
    "\n",
    "# sm_layer = SoftmaxLossLayer()\n",
    "# sm_layer.forward(bottoms, tops)\n",
    "# sm_layer.backward(bottoms, tops)\n",
    "# plt.plot(bottoms[0].data_[0, :], sm_layer.prob_[0, :], 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.0605740133\n"
     ]
    }
   ],
   "source": [
    "# if 'EuclideanLoss' in layer_creator:\n",
    "#     del layer_creator['EuclideanLoss']\n",
    "class EuclideanLossLayer:\n",
    "    '''\n",
    "    bottoms[0] of shape (batch_size * num_values), predicted values\n",
    "    bottoms[1] of shape(batch_size * num_values), ground truth values\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, param):\n",
    "        self.blobs_ = None\n",
    "        return\n",
    "    \n",
    "    def setup(self, bottoms, tops):\n",
    "        assert len(bottoms) == 2, 'layer EuclideanLossLayer only support bottoms of length 2'\n",
    "        return\n",
    "    \n",
    "    def forward(self, bottoms, tops):\n",
    "        N = bottoms[0].data_.shape[0]\n",
    "        loss = 0.5 * np.sum((bottoms[0].data_ - bottoms[1].data_) ** 2) / N\n",
    "        print loss\n",
    "        return\n",
    "    \n",
    "    def backward(self, bottoms, tops):\n",
    "        N = bottoms[0].data_.shape[0]\n",
    "        bottoms[0].diff_ = bottoms[0].data_ - bottoms[1].data_\n",
    "        return\n",
    "\n",
    "# layer_register('EuclideanLoss', EuclideanLossLayer)\n",
    "\n",
    "# bottoms = [None] * 2\n",
    "# bottoms[0] = Blob()\n",
    "# bottoms[0].data_ = np.random.randn(20, 40)\n",
    "# bottoms[1] = Blob()\n",
    "# bottoms[1].data_ = np.random.randn(20, 40)\n",
    "# tops = []\n",
    "\n",
    "# param = None\n",
    "# el_layer = EuclideanLossLayer(param)\n",
    "# el_layer.forward(bottoms, tops)\n",
    "# el_layer.backward(bottoms, tops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((10, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'Sigmoid' in layer_creator:\n",
    "    del layer_creator['Sigmoid']\n",
    "\n",
    "class SigmoidLayer:\n",
    "    def __init__(self, param):\n",
    "        self.blobs_ = None\n",
    "        return\n",
    "    \n",
    "    def setup(self, bottoms, tops):\n",
    "        assert len(bottoms) == 1, 'layer SigmoidLayer only support bottoms of length 1'\n",
    "        assert len(tops) == 1, 'layer SigmoidLayer only support tops of length 1'\n",
    "        tops[0].setup(bottoms[0].shape())\n",
    "        return\n",
    "    \n",
    "    def forward(self, bottoms, tops):\n",
    "        tops[0].data_ = 1.0 / (1.0 + np.exp(-bottoms[0].data_))\n",
    "        return\n",
    "    \n",
    "    def backward(self, bottoms, tops):\n",
    "        sig_diff = (1.0 - tops[0].data_) * tops[0].data_\n",
    "        bottoms[0].diff_ = tops[0].diff_ * sig_diff\n",
    "        return\n",
    "layer_register('Sigmoid', SigmoidLayer)\n",
    "    \n",
    "# bottoms = [None]\n",
    "# bottoms[0] = Blob()\n",
    "# bottoms[0].data_ = np.random.randn(20, 40)\n",
    "# tops = [None]\n",
    "# tops[0] = Blob()\n",
    "# tops[0].diff_ = np.ones((20, 40))\n",
    "\n",
    "# sig_param = {'name': 'sig'}\n",
    "# sig_layer = SigmoidLayer(sig_param)\n",
    "# sig_layer.forward(bottoms, tops)\n",
    "# plt.plot(bottoms[0].data_.reshape(-1), tops[0].data_.reshape(-1), 'x')\n",
    "# sig_layer.backward(bottoms, tops)\n",
    "# plt.plot(bottoms[0].data_.reshape(-1), bottoms[0].diff_.reshape(-1), 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net:\n",
    "    '''\n",
    "    Param shoud be of format [{'name': 'layer1', 'bottoms': ['blob1', 'blob2'], 'tops': ['blob1', 'blob2'], 'param': param info}, ...]\n",
    "    \n",
    "    Blob names and layer names are used by Net to bookkeeping the blobs and layers to prevent use undefined blob or \n",
    "    double define blobs\n",
    "    \n",
    "    layers use param to initilize. use bottoms and tops to setup up and initilize for internal weights and bias\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.num_layers_ = len(params)\n",
    "        self.layer_names_ = [layer['name'] for layer in params]\n",
    "        self.layers_ = []\n",
    "        \n",
    "        self.blobs_ = []\n",
    "        self.top_blobs_ = []\n",
    "        self.bottom_blobs_ = []\n",
    "        self.blob_name2idx_ = {}\n",
    "        self.blob_names_ = []\n",
    "        \n",
    "        self.learnable_params_ = []\n",
    "        \n",
    "        for param in params:\n",
    "            # initialize layer\n",
    "            layer = layer_creator[param['type']](param)\n",
    "            self.layers_.append(layer)\n",
    "            bottoms = []\n",
    "            tops = []\n",
    "            \n",
    "            # initialize bottom blobs\n",
    "            for bottom_name in param['bottoms']:\n",
    "                assert bottom_name in self.blob_names_, 'blob %s has note defined'%bottom_name\n",
    "                blob_id = self.blob_name2idx_[bottom_name]\n",
    "                bottoms.append(self.blobs_[blob_id])\n",
    "            self.bottom_blobs_.append(bottoms)\n",
    "            \n",
    "            # initialize top blobs\n",
    "            for top_name in param['tops']:\n",
    "                if top_name in self.blob_names_:\n",
    "                    if top_name in bottoms:\n",
    "                        # inplace layer, to be implemented\n",
    "                        raise NnplException('blob %s has already been defined. Not support inplace layer yet'%top_name)\n",
    "                    if top_name not in bottoms:\n",
    "                        raise NnplException('blob %s has already been defined.'%top_name)\n",
    "                else:\n",
    "                    # blob top_name has not been defined\n",
    "                    blob_id = len(self.blob_names_)\n",
    "                    self.blob_name2idx_[top_name] = blob_id\n",
    "                    self.blob_names_.append(top_name)\n",
    "                    top = Blob()\n",
    "                    tops.append(top)\n",
    "                    self.blobs_.append(top)\n",
    "            self.top_blobs_.append(tops)\n",
    "            \n",
    "            # setup layer\n",
    "            layer.setup(bottoms, tops)\n",
    "            \n",
    "            #set learnable_params\n",
    "            if layer.blobs_ != None:\n",
    "                for blob in layer.blobs_:\n",
    "                    if blob.is_update_:\n",
    "                        self.learnable_params_.append(blob)\n",
    "        return\n",
    "    \n",
    "    def forward(self):\n",
    "        loss = 0.0\n",
    "        for i in range(self.num_layers_):\n",
    "            layer = self.layers_[i]\n",
    "            layer.forward(self.bottom_blobs_[i], self.top_blobs_[i])\n",
    "#             loss += layer.forward(self.bottom_blobs_[i], self.top_blobs_[i])\n",
    "        return\n",
    "    \n",
    "    def backward(self):\n",
    "        for i in reversed(range(self.num_layers_)):\n",
    "            layer = self.layers_[i]\n",
    "            layer.backward(self.bottom_blobs_[i], self.top_blobs_[i])\n",
    "        return\n",
    "    \n",
    "    def forward_backward(self):\n",
    "        self.forward()\n",
    "        self.backward()\n",
    "        return\n",
    "    \n",
    "    def update(self):\n",
    "        for layer in self.layers_:\n",
    "            if layer.blobs_ == None:\n",
    "                continue\n",
    "            for blob in layer.blobs_:\n",
    "                blob.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.,  4.])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "b = [1, 2]\n",
    "c = [3, 4]\n",
    "np.concatenate((a, b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, param):\n",
    "        self.net_ = Net(param['net'])\n",
    "        self.type_ = param['type']\n",
    "        self.lr_rate_ = param['lr_rate']\n",
    "        self.max_iter_ = param['max_iter']\n",
    "        return\n",
    "    \n",
    "    def solve(self):\n",
    "        self.step(self.max_iter_)\n",
    "        return\n",
    "    \n",
    "    def step(self, iters):\n",
    "        for i in range(iters):\n",
    "            self.net_.forward_backward()\n",
    "            for blob in self.net_.learnable_params_:\n",
    "                blob.diff_ *= self.lr_rate_\n",
    "            self.net_.update()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txt_data_param = {}\n",
    "txt_data_param['type'] = 'TxtData'\n",
    "txt_data_param['tops'] = ['features', 'labels']\n",
    "txt_data_param['bottoms'] = []\n",
    "txt_data_param['batch_size'] = 150\n",
    "txt_data_param['feature_file'] = './iris/features.dat'\n",
    "txt_data_param['label_file'] = './iris/labels.dat'\n",
    "txt_data_param['num_entries'] = 150\n",
    "txt_data_param['name'] = 'Data'\n",
    "\n",
    "weight_filler_param1 = {'type': 'gaussian', 'std': 0.01}\n",
    "bias_filler_param1 = {'type': 'constant'}\n",
    "ip_param1 = {}\n",
    "ip_param1['type'] = 'InnerProduct'\n",
    "ip_param1['tops'] = ['ip1']\n",
    "ip_param1['bottoms'] = ['features']\n",
    "ip_param1['output_size'] = 100\n",
    "ip_param1['name'] = 'ip1'\n",
    "ip_param1['weight_filler_param'] = weight_filler_param1\n",
    "ip_param1['bias_filler_param'] = bias_filler_param1\n",
    "\n",
    "sig_param = {}\n",
    "sig_param['type'] = 'Sigmoid'\n",
    "sig_param['name'] = 'sig'\n",
    "sig_param['bottoms'] = ['ip1']\n",
    "sig_param['tops'] = ['sig']\n",
    "\n",
    "weight_filler_param2 = {'type': 'gaussian', 'std': 0.01}\n",
    "bias_filler_param2 = {'type': 'constant'}\n",
    "ip_param2 = {}\n",
    "ip_param2['type'] = 'InnerProduct'\n",
    "ip_param2['tops'] = ['ip2']\n",
    "ip_param2['bottoms'] = ['sig']\n",
    "ip_param2['output_size'] = 3\n",
    "ip_param2['name'] = 'ip2'\n",
    "ip_param2['weight_filler_param'] = weight_filler_param2\n",
    "ip_param2['bias_filler_param'] = bias_filler_param2\n",
    "\n",
    "sm_param = {}\n",
    "sm_param['type'] = 'SoftmaxLoss'\n",
    "sm_param['name'] = 'softmax'\n",
    "sm_param['bottoms'] = ['ip2', 'labels']\n",
    "sm_param['tops'] = []\n",
    "\n",
    "net_param = [txt_data_param, ip_param1, sig_param, ip_param2, sm_param]\n",
    "\n",
    "solver_param = {'net': net_param, 'lr_rate': 0.5, 'max_iter': 500, 'type': 'sgd'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = Solver(solver_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.09920399189\n",
      "1.10251673785\n",
      "1.14750381477\n",
      "1.54160233348\n",
      "2.44239670706\n",
      "1.95529456788\n",
      "1.6454781027\n",
      "1.27735486609\n",
      "1.1375970113\n",
      "1.10741357534\n",
      "1.08901278448\n",
      "1.08087637066\n",
      "1.0725020127\n",
      "1.06564983673\n",
      "1.0581913356\n",
      "1.05022905554\n",
      "1.04109363148\n",
      "1.03061340232\n",
      "1.01837828349\n",
      "1.00419398215\n",
      "0.987777134588\n",
      "0.969083353244\n",
      "0.948089359026\n",
      "0.925040803972\n",
      "0.900195252518\n",
      "0.873996604048\n",
      "0.846826653748\n",
      "0.819171971456\n",
      "0.791417167972\n",
      "0.764000654137\n",
      "0.737237810701\n",
      "0.711455610543\n",
      "0.686850711593\n",
      "0.663603020556\n",
      "0.641777716949\n",
      "0.621417286068\n",
      "0.602481568939\n",
      "0.584923514415\n",
      "0.568647232425\n",
      "0.5535776847\n",
      "0.539615912569\n",
      "0.526701006492\n",
      "0.514769464356\n",
      "0.503801589673\n",
      "0.493837087434\n",
      "0.485015554402\n",
      "0.477734746784\n",
      "0.472787123062\n",
      "0.47177288354\n",
      "0.477505205232\n",
      "0.493526324544\n",
      "0.522108033747\n",
      "0.556192246642\n",
      "0.577590769663\n",
      "0.581015792816\n",
      "0.55506164565\n",
      "0.53920530449\n",
      "0.506585151223\n",
      "0.492711261562\n",
      "0.469564005118\n",
      "0.459734389319\n",
      "0.444462626771\n",
      "0.437857593898\n",
      "0.427627178084\n",
      "0.423494712614\n",
      "0.416580642889\n",
      "0.414520014645\n",
      "0.409999015795\n",
      "0.409786782932\n",
      "0.407051058416\n",
      "0.408381639139\n",
      "0.40676027754\n",
      "0.409022022685\n",
      "0.407615169461\n",
      "0.40990404946\n",
      "0.407781811809\n",
      "0.409263066825\n",
      "0.405907446804\n",
      "0.406228283048\n",
      "0.40175312996\n",
      "0.401090285866\n",
      "0.396025088685\n",
      "0.39480724328\n",
      "0.389731301941\n",
      "0.388362638021\n",
      "0.383687847545\n",
      "0.38242621657\n",
      "0.378357134842\n",
      "0.377305270096\n",
      "0.373882299107\n",
      "0.37302149726\n",
      "0.370178895379\n",
      "0.369411465013\n",
      "0.36702856102\n",
      "0.366220890022\n",
      "0.364165207336\n",
      "0.363188165486\n",
      "0.36134979533\n",
      "0.360109723856\n",
      "0.358422528895\n",
      "0.356875551337\n",
      "0.355321649174\n",
      "0.353469723326\n",
      "0.352069279273\n",
      "0.349943816783\n",
      "0.348737267616\n",
      "0.346379517737\n",
      "0.345410122632\n",
      "0.342855657328\n",
      "0.342157366708\n",
      "0.339427796236\n",
      "0.339019745674\n",
      "0.336121274252\n",
      "0.336007764883\n",
      "0.332934447122\n",
      "0.333108358449\n",
      "0.329847569037\n",
      "0.330295192871\n",
      "0.326833201192\n",
      "0.327538926989\n",
      "0.323865143054\n",
      "0.324814984543\n",
      "0.320924202278\n",
      "0.32210772378\n",
      "0.318000387214\n",
      "0.31941108718\n",
      "0.315092107926\n",
      "0.31672666498\n",
      "0.312203540715\n",
      "0.314060475067\n",
      "0.309341409649\n",
      "0.31141967536\n",
      "0.306512176487\n",
      "0.308810045425\n",
      "0.303720197547\n",
      "0.30623460538\n",
      "0.300966980122\n",
      "0.303693340338\n",
      "0.298251361256\n",
      "0.301183745192\n",
      "0.295570272437\n",
      "0.298701805344\n",
      "0.292919727576\n",
      "0.296243051379\n",
      "0.290295736855\n",
      "0.293803422479\n",
      "0.287694959755\n",
      "0.291379798461\n",
      "0.285115027404\n",
      "0.288970177506\n",
      "0.282554559976\n",
      "0.286573562735\n",
      "0.280012964821\n",
      "0.284189666399\n",
      "0.277490123442\n",
      "0.28181854815\n",
      "0.27498606715\n",
      "0.27946028368\n",
      "0.272500713873\n",
      "0.277114725264\n",
      "0.270033704199\n",
      "0.274781378505\n",
      "0.267584342919\n",
      "0.27245938859\n",
      "0.265151628919\n",
      "0.270147609055\n",
      "0.262734343426\n",
      "0.267844717178\n",
      "0.260331163539\n",
      "0.26554934096\n",
      "0.257940772243\n",
      "0.263260169956\n",
      "0.255561944671\n",
      "0.260976032752\n",
      "0.253193600263\n",
      "0.258695934639\n",
      "0.250834819529\n",
      "0.256419058022\n",
      "0.248484830827\n",
      "0.254144734091\n",
      "0.246142976525\n",
      "0.251872397237\n",
      "0.243808669101\n",
      "0.249601533766\n",
      "0.241481346798\n",
      "0.247331634597\n",
      "0.239160436183\n",
      "0.245062158626\n",
      "0.236845326016\n",
      "0.242792510153\n",
      "0.234535354012\n",
      "0.24052203073\n",
      "0.232229805704\n",
      "0.238250003544\n",
      "0.229927922982\n",
      "0.235975666999\n",
      "0.227628918996\n",
      "0.233698233583\n",
      "0.225331995963\n",
      "0.231416910221\n",
      "0.223036362769\n",
      "0.229130916942\n",
      "0.220741249935\n",
      "0.226839501598\n",
      "0.218445920411\n",
      "0.224541949327\n",
      "0.216149675468\n",
      "0.222237586434\n",
      "0.213851855764\n",
      "0.219925779053\n",
      "0.211551838159\n",
      "0.217605927547\n",
      "0.20924902928\n",
      "0.215277457858\n",
      "0.206942856975\n",
      "0.21293981116\n",
      "0.204632760804\n",
      "0.210592433055\n",
      "0.202318182596\n",
      "0.208234763391\n",
      "0.199998557916\n",
      "0.205866227514\n",
      "0.197673309011\n",
      "0.203486229482\n",
      "0.195341839612\n",
      "0.201094147532\n",
      "0.193003531717\n",
      "0.198689331838\n",
      "0.190657744333\n",
      "0.196271104453\n",
      "0.188303814039\n",
      "0.193838761236\n",
      "0.185941057152\n",
      "0.191391575487\n",
      "0.183568773293\n",
      "0.18892880309\n",
      "0.18118625018\n",
      "0.186449688978\n",
      "0.178792769558\n",
      "0.183953474878\n",
      "0.17638761428\n",
      "0.181439408443\n",
      "0.173970076724\n",
      "0.178906754021\n",
      "0.171539468841\n",
      "0.176354805538\n",
      "0.169095134359\n",
      "0.173782902136\n",
      "0.16663646381\n",
      "0.171190447453\n",
      "0.164162913252\n",
      "0.168576933608\n",
      "0.161674027717\n",
      "0.165941971153\n",
      "0.159169470606\n",
      "0.163285326397\n",
      "0.156649060307\n",
      "0.160606967603\n",
      "0.154112815415\n",
      "0.157907121521\n",
      "0.151561009778\n",
      "0.155186341531\n",
      "0.14899423841\n",
      "0.152445588244\n",
      "0.146413494725\n",
      "0.149686322659\n",
      "0.143820258773\n",
      "0.146910610786\n",
      "0.14121659488\n",
      "0.144121237007\n",
      "0.138605255369\n",
      "0.141321821241\n",
      "0.135989784915\n",
      "0.138516932319\n",
      "0.133374617478\n",
      "0.135712187008\n",
      "0.130765155126\n",
      "0.132914321239\n",
      "0.128167815648\n",
      "0.130131217825\n",
      "0.125590034358\n",
      "0.127371874175\n",
      "0.123040205658\n",
      "0.124646294896\n",
      "0.12052755242\n",
      "0.121965298681\n",
      "0.118061916705\n",
      "0.11934023661\n",
      "0.115653473588\n",
      "0.1167826297\n",
      "0.113312380305\n",
      "0.114303745772\n",
      "0.111048383814\n",
      "0.111914147339\n",
      "0.108870419031\n",
      "0.109623250491\n",
      "0.106786235084\n",
      "0.107438937403\n",
      "0.104802086231\n",
      "0.105367260697\n",
      "0.102922517157\n",
      "0.103412266949\n",
      "0.101150260447\n",
      "0.101575951192\n",
      "0.0994862494608\n",
      "0.0998583376091\n",
      "0.097929735802\n",
      "0.0982576670835\n",
      "0.0964784896301\n",
      "0.0967706624965\n",
      "0.0951290549349\n",
      "0.0953928386388\n",
      "0.0938770308126\n",
      "0.0941188250538\n",
      "0.0927173528902\n",
      "0.0929426755201\n",
      "0.0916445548195\n",
      "0.0918581453588\n",
      "0.0906529965662\n",
      "0.0908589255531\n",
      "0.0897370527166\n",
      "0.0899388295166\n",
      "0.0888912593679\n",
      "0.0890919335815\n",
      "0.0881104220014\n",
      "0.0883126757266\n",
      "0.0873896890823\n",
      "0.0875959189177\n",
      "0.0867245972265\n",
      "0.0869369860454\n",
      "0.0861110939542\n",
      "0.0863316732078\n",
      "0.0855455436437\n",
      "0.0857762473589\n",
      "0.0850247215698\n",
      "0.0852674333845\n",
      "0.0845458000645\n",
      "0.0848023946772\n",
      "0.0841063300032\n",
      "0.0843787103555\n",
      "0.0837042200676\n",
      "0.0839943514873\n",
      "0.0833377156059\n",
      "0.0836476580293\n",
      "0.0830053784074\n",
      "0.0833373177072\n",
      "0.0827060683282\n",
      "0.0830623476962\n",
      "0.0824389274313\n",
      "0.0828220797216\n",
      "0.0822033671264\n",
      "0.0826161490527\n",
      "0.0819990586947\n",
      "0.0824444878054\n",
      "0.0818259275556\n",
      "0.0823073229855\n",
      "0.081684151669\n",
      "0.0822051797941\n",
      "0.0815741645622\n",
      "0.0821388908858\n",
      "0.0814966636446\n",
      "0.0821096125254\n",
      "0.0814526247212\n",
      "0.082118848963\n",
      "0.0814433239802\n",
      "0.0821684868661\n",
      "0.0814703692417\n",
      "0.082260842391\n",
      "0.0815357429731\n",
      "0.0823987245189\n",
      "0.0816418606087\n",
      "0.0825855197842\n",
      "0.0817916491965\n",
      "0.0828253057164\n",
      "0.0819886535782\n",
      "0.0831230035484\n",
      "0.0822371805615\n",
      "0.0834845856287\n",
      "0.082542496492\n",
      "0.0839173604635\n",
      "0.0829111013094\n",
      "0.0844303700701\n",
      "0.0833511143528\n",
      "0.0850349531899\n",
      "0.0838728269916\n",
      "0.08574555901\n",
      "0.0844895102861\n",
      "0.0865809487826\n",
      "0.085218623008\n",
      "0.0875660151692\n",
      "0.0860836673701\n",
      "0.0887346172269\n",
      "0.0871171293925\n",
      "0.0901341478306\n",
      "0.0883653095848\n",
      "0.091833185553\n",
      "0.0898966059652\n",
      "0.0939349227359\n",
      "0.0918164624542\n",
      "0.0966020843329\n",
      "0.0942960771419\n",
      "0.100106449113\n",
      "0.0976319470096\n",
      "0.104936076749\n",
      "0.102382003631\n",
      "0.112054545425\n",
      "0.10971884654\n",
      "0.123626517191\n",
      "0.122515696588\n",
      "0.145503520193\n",
      "0.149563715652\n",
      "0.198588066945\n",
      "0.227959351898\n",
      "0.394460875066\n",
      "0.567831587347\n",
      "1.32453434938\n",
      "0.593419661097\n",
      "0.981427888525\n",
      "0.47165334919\n",
      "0.548107763797\n",
      "0.325213350911\n",
      "0.285341873279\n",
      "0.170884928454\n",
      "0.127892358922\n",
      "0.0938905113954\n",
      "0.0836853432512\n",
      "0.0793830869965\n",
      "0.0779059022947\n",
      "0.0771316592506\n",
      "0.0766020806116\n",
      "0.0761556580768\n",
      "0.075748139624\n",
      "0.0753651261441\n",
      "0.0750011277243\n",
      "0.0746536551814\n",
      "0.0743210222515\n",
      "0.0740019870978\n",
      "0.0736954630233\n",
      "0.0734005145097\n",
      "0.0731162976796\n",
      "0.0728420618973\n",
      "0.0725771291674\n",
      "0.0723208902259\n",
      "0.0720727939123\n",
      "0.0718323419662\n",
      "0.0715990821527\n",
      "0.0713726034923\n",
      "0.0711525313237\n",
      "0.0709385233723\n",
      "0.07073026605\n",
      "0.0705274713407\n",
      "0.0703298739828\n",
      "0.0701372290424\n",
      "0.0699493097543\n",
      "0.0697659056434\n",
      "0.0695868208651\n",
      "0.0694118727528\n",
      "0.0692408905379\n",
      "0.0690737142248\n",
      "0.0689101935984\n",
      "0.0687501873493\n",
      "0.0685935622991\n",
      "0.0684401927156\n",
      "0.0682899597033\n",
      "0.0681427506627\n",
      "0.0679984588061\n",
      "0.0678569827264\n",
      "0.0677182260089\n",
      "0.0675820968825\n",
      "0.0674485079055\n",
      "0.0673173756804\n",
      "0.0671886205961\n",
      "0.067062166592\n",
      "0.0669379409439\n",
      "0.066815874067\n",
      "0.0666958993357\n",
      "0.0665779529172\n",
      "0.0664619736179\n",
      "0.0663479027421\n",
      "0.0662356839599\n",
      "0.0661252631849\n",
      "0.0660165884604\n",
      "0.0659096098528\n",
      "0.0658042793526\n",
      "0.0657005507808\n",
      "0.0655983797017\n",
      "0.0654977233409\n",
      "0.0653985405079\n",
      "0.0653007915238\n",
      "0.0652044381519\n",
      "0.065109443534\n",
      "0.0650157721283\n",
      "0.0649233896521\n",
      "0.0648322630265\n",
      "0.0647423603252\n",
      "0.0646536507246\n",
      "0.0645661044573\n",
      "0.0644796927682\n",
      "0.0643943878715\n",
      "0.0643101629114\n"
     ]
    }
   ],
   "source": [
    "s.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features', 'labels', 'ip1', 'sig', 'ip2']"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.net_.blob_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gt = s.net_.blobs_[s.net_.blob_name2idx_['labels']].data_\n",
    "# n.blobs_[n.blob_name2idx_['features']].diff_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(s.net_.blobs_[s.net_.blob_name2idx_['ip2']].data_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pred == gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = [np]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nnpl import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
