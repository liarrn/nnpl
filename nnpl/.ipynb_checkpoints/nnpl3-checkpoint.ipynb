{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "import time\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# splite cluster configurations in source file into seperate files for each cluster in destination directory\n",
    "\n",
    "def splite_coor_file(source, destination):\n",
    "    with open(source) as fp:\n",
    "        lines = fp.readlines()\n",
    "    ln = 0\n",
    "    conf = 0\n",
    "\n",
    "    while ln < len(lines):\n",
    "        natom = int(lines[ln])\n",
    "        with open(destination+'%d.xyz'%conf, 'w') as fp:\n",
    "            fp.write(''.join(lines[ln: ln+natom+2]))\n",
    "        ln = ln + natom + 2\n",
    "        conf += 1\n",
    "\n",
    "#splite_coor_file('./subset.xyz', './coor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nnpl.transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time when finished reading xyz data 0.0360000133514\n",
      "time when finished transforming and writing 34.7449998856\n"
     ]
    }
   ],
   "source": [
    "source_file_path = './nnpl/tools/subset-0.xyz'\n",
    "feature_file_path = './nnpl/tools/features.dat'\n",
    "label_file_path = './nnpl/tools/labels.dat'\n",
    "\n",
    "param = {}\n",
    "param['radial'] = {'eta_list':  [0.12, 0.2], 'rs_list': [0.0], 'rc_list': [6.0]}\n",
    "param['angle'] = {'eta_list':  [0.03, 0.09], 'lambda_list':  [-1.0, 1.0],\n",
    "                  'zeta_list': [1.0, 2.0, 4.0], 'rc_list': [6.0]}\n",
    "dt = BehlerDataTransform(param)\n",
    "\n",
    "start_time = time.time()\n",
    "with open(source_file_path) as fp:\n",
    "    lines = fp.readlines()\n",
    "print 'time when finished reading xyz data', time.time() - start_time\n",
    "ln = 0\n",
    "while ln < len(lines):\n",
    "    natom = int(lines[ln])\n",
    "    energy = float(lines[ln+1].split()[-2])\n",
    "    label = np.array([[natom, energy]])\n",
    "    coor = np.array([map(float, line.split()[1: 4]) for line in lines[ln+2: ln + natom + 2]])\n",
    "    feature = dt.transform(coor)\n",
    "    ln = ln + natom + 2\n",
    "    with open(feature_file_path, 'a') as fp:\n",
    "        with open(label_file_path, 'a') as lp:\n",
    "            np.savetxt(fp, feature, delimiter=',', fmt='%.8f', newline='\\n')\n",
    "            np.savetxt(lp, label, delimiter=',', fmt='%.8f', newline='\\n')\n",
    "    \n",
    "print 'time when finished transforming and writing', time.time() - start_time\n",
    "\n",
    "# start_time = time.time()\n",
    "# with open(source_file_path) as fp:\n",
    "#     lines = fp.readlines()\n",
    "# print 'time when finished reading xyz data', time.time() - start_time\n",
    "# ln = 0\n",
    "# accum_natoms = 0  # accumulated natoms\n",
    "# labels = np.empty((0, 2), dtype=float)\n",
    "# features = np.empty((0, dt.infer_feature_size()), dtype=float)\n",
    "# while ln < len(lines):\n",
    "#     natom = int(lines[ln])\n",
    "#     energy = float(lines[ln+1].split()[-2])\n",
    "#     label = np.array([[natom, energy]])\n",
    "#     coor = np.array([map(float, line.split()[1: 4]) for line in lines[ln+2: ln + natom + 2]])\n",
    "#     feature = dt.transform(coor)\n",
    "#     features = np.concatenate((features, feature), axis=0)\n",
    "#     labels = np.concatenate((labels, label), axis=0)\n",
    "#     ln = ln + natom + 2\n",
    "    \n",
    "#     accum_natoms += natom\n",
    "#     if accum_natoms >= 10000:\n",
    "#         accum_natoms = 0\n",
    "#         with open(feature_file_path, 'a') as fp:\n",
    "#             with open(label_file_path, 'a') as lp:\n",
    "#                 np.savetxt(fp, features, delimiter=',', fmt='%.8f', newline='\\n')\n",
    "#                 np.savetxt(lp, labels, delimiter=',', fmt='%.8f', newline='\\n')\n",
    "    \n",
    "# print 'time when finished transforming and writing', time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.33368743e+00,  -2.46812007e+00,  -1.15490280e+00],\n",
       "       [  3.34824707e-01,  -1.03038158e+00,   8.82182857e-01],\n",
       "       [ -3.41980241e-01,   4.96956666e-01,  -1.61157850e+00],\n",
       "       [ -2.59317470e-01,  -1.73891070e+00,   3.44092452e+00],\n",
       "       [  1.93232180e+00,   2.51950159e+00,  -1.81054965e+00],\n",
       "       [ -1.85420201e+00,   8.92642573e-01,   5.76306788e-01],\n",
       "       [  8.13404107e-01,   2.03384444e+00,   6.34631113e-01],\n",
       "       [ -1.94038699e-01,   9.09199470e-01,   2.87970987e+00],\n",
       "       [  2.34921607e+00,   3.28195700e-03,  -9.21672177e-01]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./coor/100.xyz', 'r') as fp:\n",
    "    coor = fp.readlines()\n",
    "coor = coor[2: ]\n",
    "coor = np.array([map(float, line.split()[1: 4]) for line in coor])\n",
    "coor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make grid of list of arguments\n",
    "# suppose we have 4 arguments, each has [M, N, P, Q] entries\n",
    "# the q arguments vary the fastest\n",
    "# for an arbitrary permute [m, n, p, q], the index is \n",
    "# q + pQ + nPQ + mNPQ = q+Q(p+P(n+mN))\n",
    "def make_grid(*args):\n",
    "    num_var = len(args)\n",
    "    num_permut = reduce(lambda count, arg: count * len(arg), args, 1)\n",
    "    result = np.zeros((num_permut, num_var))\n",
    "    for i in range(num_permut):\n",
    "        index = i\n",
    "        for j in reversed(range(num_var)):\n",
    "            sub_index = index % len(args[j])\n",
    "            result[i, j] = args[j][sub_index]\n",
    "            index /= len(args[j])\n",
    "    return result\n",
    "\n",
    "# a =  range(0, 2)\n",
    "# b = range(2, 4)\n",
    "# c = range(4, 6)\n",
    "# make_grid(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.]\n",
      " [ 1.  1.  1.]\n",
      " [ 1.  1.  1.]]\n",
      "[[ 1.  2.  3.]\n",
      " [ 1.  2.  3.]\n",
      " [ 1.  2.  3.]]\n",
      "[[ 1.  1.  1.]\n",
      " [ 2.  2.  2.]\n",
      " [ 3.  3.  3.]]\n"
     ]
    }
   ],
   "source": [
    "# a = np.random.randint(10, size=9).reshape((3, 3))\n",
    "a = np.ones((3, 3))\n",
    "print a\n",
    "print a + np.arange(3).reshape((1, 3))\n",
    "print a + np.arange(3).reshape((3, 1))\n",
    "# a[(a > 5)] = 2 * a[(a > 5)]\n",
    "# a[(a < 5)] = 0\n",
    "# np.fill_diagonal(a, 1)\n",
    "# print a\n",
    "# (a-1).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00300002098083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BehlerDataTransform:\n",
    "    '''\n",
    "    TODO: lots of optimization \n",
    "    TODO: only support single rc cutoff for both angular symm func and radial symm func\n",
    "    Adopted from Behler et. al. PRL 98, 146401 (2007)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, param):\n",
    "        assert len(param['radial']['rc_list']) == 1 and len(param['angle']['rc_list']) ==1 and \\\n",
    "            param['angle']['rc_list'] == param['radial']['rc_list'], \\\n",
    "            'only support single rc'\n",
    "        self.radial_param_ = copy.deepcopy(param['radial'])\n",
    "        self.angle_param_ = copy.deepcopy(param['angle'])\n",
    "        self.dist_mat_ = None\n",
    "        self.soft_cutoff_mat_ = None\n",
    "        self.cos_mat_ = None\n",
    "        self.radial_symm_func_ = None\n",
    "        self.angular_symm_func_ = None\n",
    "        return\n",
    "    \n",
    "    def setup(self, coor):\n",
    "        self.calc_dist_mat(coor)\n",
    "        self.calc_soft_cutoff_mat(coor)\n",
    "        self.calc_cos_mat(coor)\n",
    "        return\n",
    "    \n",
    "    def reset(self):\n",
    "        self.dist_mat_ = None\n",
    "        self.soft_cutoff_mat_ = None\n",
    "        self.cos_mat_ = None\n",
    "        self.radial_symm_func_ = None\n",
    "        self.angular_symm_func_ = None\n",
    "        return\n",
    "    \n",
    "    def infer_feature_size(self):\n",
    "        radial_size = reduce(lambda count, item: count * len(item), self.radial_param_.values(), 1)\n",
    "        angular_size = reduce(lambda count, item: count * len(item), self.angle_param_.values(), 1)\n",
    "        return radial_size + angular_size\n",
    "    \n",
    "    def calc_soft_cutoff_mat(self, coor):\n",
    "        ''' \n",
    "        calculate soft cutoff matrix with respect of inter-atom length r\n",
    "        '''\n",
    "        assert self.dist_mat_ != None, 'dis_mat_ should be calculated first'\n",
    "        rc = param['radial']['rc_list'][0]\n",
    "        self.soft_cutoff_mat_ = np.zeros_like(self.dist_mat_)\n",
    "        self.soft_cutoff_mat_[self.dist_mat_ <= rc] = 0.5 * (np.cos(np.pi * self.dist_mat_[self.dist_mat_ <= rc] / rc) + 1)\n",
    "        return\n",
    "    \n",
    "    def calc_dist_mat(self, coor):\n",
    "        '''\n",
    "        calculate distance matrix\n",
    "        '''\n",
    "        d = pdist(coor, metric='euclidean')\n",
    "        self.dist_mat_ = squareform(d)\n",
    "        return\n",
    "    \n",
    "    def calc_cos_mat (self, coor):\n",
    "        '''\n",
    "        calculate the cosine matrix \n",
    "        '''\n",
    "        assert self.dist_mat_ != None, 'dis_mat_ should be calculated first'\n",
    "        N = coor.shape[0]\n",
    "        self.cos_mat_ = np.empty((N, N, N))\n",
    "        for i in range(N):\n",
    "            numerator = np.dot(coor - coor[i, :], (coor - coor[i, :]).T)\n",
    "            denominator = np.dot(self.dist_mat_[i, :].reshape((N, 1)), self.dist_mat_[i, :].reshape((1, N)))\n",
    "            denominator[i, :] = 1 # mute divide zero error\n",
    "            denominator[:, i] = 1 #  mute divide zero error\n",
    "            self.cos_mat_[i] = numerator / denominator\n",
    "            self.cos_mat_[i][i, :] = 0\n",
    "            self.cos_mat_[i][:, i] = 0\n",
    "        return\n",
    "    \n",
    "    def calc_radial_symm_func(self, coor):\n",
    "        '''\n",
    "        Radial symmetry functions\n",
    "        '''\n",
    "        assert self.dist_mat_ != None, 'dis_mat_ should be calculated first'\n",
    "        assert self.soft_cutoff_mat_ != None, 'soft_cutoff_mat_ should be calculated first'\n",
    "        args_list = make_grid(self.radial_param_['eta_list'], self.radial_param_['rs_list'], self.radial_param_['rc_list'])\n",
    "        N = coor.shape[0]\n",
    "        self.radial_symm_func_ = np.empty((N, len(args_list)))\n",
    "        for args_num in range(len(args_list)):\n",
    "            eta, rs, rc = args_list[args_num]\n",
    "            dist_mat = self.dist_mat_.copy()\n",
    "            soft_cutoff_mat = self.soft_cutoff_mat_.copy()\n",
    "            np.fill_diagonal(dist_mat, rs)\n",
    "            np.fill_diagonal(soft_cutoff_mat, 0)\n",
    "            self.radial_symm_func_[:, args_num] = np.sum(np.exp(-eta * (dist_mat - rs) ** 2) * soft_cutoff_mat, axis=1)\n",
    "        return\n",
    "    \n",
    "    def calc_angular_symm_func(self, coor):\n",
    "        '''\n",
    "        Angular symmetry functions\n",
    "        '''\n",
    "        assert self.dist_mat_ != None, 'dis_mat_ should be calculated first'\n",
    "        assert self.soft_cutoff_mat_ != None, 'soft_cutoff_mat_ should be calculated first'\n",
    "        assert self.cos_mat_ != None, 'cos_mat_ should be calculated first'\n",
    "        args_list = make_grid(self.angle_param_['eta_list'], self.angle_param_['lambda_list'], \n",
    "                              self.angle_param_['zeta_list'], self.angle_param_['rc_list']) \n",
    "        N = coor.shape[0]\n",
    "        self.angular_symm_func_ = np.empty((N, len(args_list)))\n",
    "        square_dist_mat = self.dist_mat_ ** 2\n",
    "        \n",
    "        for i in range(N):\n",
    "            fc = self.soft_cutoff_mat_ * self.soft_cutoff_mat_[i, :].reshape((1, N)) * self.soft_cutoff_mat_[i, :].reshape((N, 1))\n",
    "            fc[i, :] = 0\n",
    "            fc[:, i] = 0\n",
    "            dsq = square_dist_mat + square_dist_mat[i, :].reshape((1, N)) + square_dist_mat[i, :].reshape((N, 1))\n",
    "            dsq[i, :] = 0\n",
    "            dsq[:, i] = 0\n",
    "            for args_num in range(len(args_list)):\n",
    "                eta, lambda_, zeta, rc = args_list[args_num]\n",
    "                self.angular_symm_func_[i, args_num] = 2 ** (1-zeta) * np.sum((1+lambda_*self.cos_mat_[i])**zeta * np.exp(-eta*dsq) * fc )\n",
    "        return\n",
    "    \n",
    "    def transform(self, coor):\n",
    "        self.setup(coor)\n",
    "        self.calc_radial_symm_func(coor)\n",
    "        self.calc_angular_symm_func(coor)\n",
    "        result = np.hstack((self.radial_symm_func_, self.angular_symm_func_))\n",
    "        self.reset()\n",
    "        return result\n",
    "\n",
    "\n",
    "with open('./coor/100.xyz', 'r') as fp:\n",
    "    coor = fp.readlines()\n",
    "coor = coor[2: ]\n",
    "coor = np.array([map(float, line.split()[1: 4]) for line in coor])\n",
    "param = {}\n",
    "param['radial'] = {'eta_list':  [0.12, 0.2], 'rs_list': [0.0], 'rc_list': [6.0]}\n",
    "param['angle'] = {'eta_list':  [0.03, 0.09], 'lambda_list':  [-1.0, 1.0],\n",
    "                  'zeta_list': [1.0, 2.0, 4.0], 'rc_list': [6.0]}\n",
    "dt = BehlerDataTransform(param)\n",
    "start_time = time.time()\n",
    "a = dt.transform(coor)\n",
    "elapsed_time = time.time() - start_time\n",
    "print elapsed_time\n",
    "dt.infer_feature_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.427999973297\n"
     ]
    }
   ],
   "source": [
    "class DataTransform:\n",
    "    '''TODO: lots of optimization '''\n",
    "    \n",
    "    def __init__(self, param):\n",
    "        self.radial_param_ = copy.deepcopy(param['radial'])\n",
    "        self.angle_param_ = copy.deepcopy(param['angle'])\n",
    "        self.dist_mat = None\n",
    "        return\n",
    "    \n",
    "    def infer_feature_size(self):\n",
    "        radial_size = reduce(lambda count, item: count * len(item), self.radial_param_.values(), 1)\n",
    "        angular_size = reduce(lambda count, item: count * len(item), self.angle_param_.values(), 1)\n",
    "        return radial_size + angular_size\n",
    "    \n",
    "    def soft_cutoff(self, r, rc):\n",
    "        ''' \n",
    "        a soft cutoff function with respect of inter-atom length r\n",
    "        from Behler et. al. PRL 98, 146401 (2007)\n",
    "        '''\n",
    "        if r > rc:\n",
    "            return 0\n",
    "        if r <= rc:\n",
    "            return 0.5 * (np.cos(np.pi * r / rc) + 1)\n",
    "        \n",
    "    def dist(self, a1, a2):\n",
    "        '''distance between two atoms'''\n",
    "        return np.sqrt(np.sum((a1-a2)**2))\n",
    "    \n",
    "    def angle (self, ca, ra1, ra2):\n",
    "        '''angle between center atom and two reference atoms'''\n",
    "        b1 = ra1 - ca\n",
    "        b2 = ra2 - ca\n",
    "        tmp = np.dot(b1, b2) / (self.dist(ca, ra1) * self.dist(ca, ra2))\n",
    "        tmp = np.clip(tmp, -1, 1)\n",
    "        return np.arccos(tmp)\n",
    "    \n",
    "    def transform(self, coor):\n",
    "        return np.hstack((self.radial_symm_func(coor), self.angular_symm_func(coor)))\n",
    "    \n",
    "    def radial_symm_func(self, coor):\n",
    "        '''Radial symmetry functions'''\n",
    "        #         args_list = make_grid(eta_list, rs_list, rc_list)\n",
    "        args_list = make_grid(self.radial_param_['eta_list'], self.radial_param_['rs_list'], self.radial_param_['rc_list'])\n",
    "        num_atoms = coor.shape[0]\n",
    "        result = np.zeros((num_atoms, len(args_list)))\n",
    "        for center_atom in range(num_atoms):\n",
    "            for args_num in range(len(args_list)):\n",
    "                eta, rs, rc = args_list[args_num]\n",
    "                for ref_atom in range(num_atoms):\n",
    "                    # ra stands for reference atom\n",
    "                    if ref_atom == center_atom:\n",
    "                        continue\n",
    "                    dist_ref_center = self.dist(coor[ref_atom], coor[center_atom])\n",
    "                    # symmetry function between center atom and reference atom\n",
    "                    gcr = self.soft_cutoff(dist_ref_center, rc) * np.exp(-eta*(dist_ref_center-rs)**2)\n",
    "                    result[center_atom, args_num]  += gcr\n",
    "        return result\n",
    "    \n",
    "    def angular_symm_func(self, coor):\n",
    "        '''Angular symmetry functions'''\n",
    "        #         args_list = make_grid(eta_list, lambda_list, zeta_list, rc_list)\n",
    "        args_list = make_grid(self.angle_param_['eta_list'], self.angle_param_['lambda_list'], \n",
    "                              self.angle_param_['zeta_list'], self.angle_param_['rc_list']) \n",
    "        num_atoms = coor.shape[0]\n",
    "        result = np.zeros((num_atoms, len(args_list)))\n",
    "        for center_atom in range(num_atoms):\n",
    "            for args_num in range(len(args_list)):\n",
    "                eta, lambda_, zeta, rc = args_list[args_num]\n",
    "                for ref_atom_1 in range(num_atoms):\n",
    "                    for ref_atom_2 in range(num_atoms):\n",
    "                        if ref_atom_1 == center_atom or ref_atom_2 == center_atom:\n",
    "                            continue\n",
    "                        dist_ref1_center = self.dist(coor[ref_atom_1], coor[center_atom])\n",
    "                        dist_ref2_center = self.dist(coor[ref_atom_2], coor[center_atom])\n",
    "                        dist_ref1_ref2 = self.dist(coor[ref_atom_1], coor[ref_atom_2])\n",
    "                        # symmetry function \n",
    "                        if center_atom == 0:\n",
    "                            self.angle(coor[center_atom], coor[ref_atom_1], coor[ref_atom_2])\n",
    "#                             print np.cos(self.angle(coor[center_atom], coor[ref_atom_1], coor[ref_atom_2]))\n",
    "                        gcr = self.soft_cutoff(dist_ref1_center, rc) * self.soft_cutoff(dist_ref2_center, rc) * self.soft_cutoff(dist_ref1_ref2, rc)\n",
    "                        gcr *= 2 ** (1 - zeta)\n",
    "                        gcr *= (1 + lambda_ * np.cos(self.angle(coor[center_atom], coor[ref_atom_1], coor[ref_atom_2]))) ** zeta\n",
    "                        gcr *= np.exp(-eta*(dist_ref1_center**2+dist_ref2_center**2+dist_ref1_ref2**2))\n",
    "                        result[center_atom, args_num]  += gcr\n",
    "        return result\n",
    "\n",
    "\n",
    "eta_list = [0.03, 0.09]\n",
    "lambda_list = [-1.0, 1.0]\n",
    "zeta_list = [1.0, 2.0, 4.0]\n",
    "rc_list = [6.0]\n",
    "# angular_symm_func(coor, eta_list, lambda_list, zeta_list, rc_list)\n",
    "\n",
    "eta_list = [0.12, 0.2]\n",
    "rs_list = [0.0]\n",
    "rc_list = [6.0]\n",
    "# radial_symm_func(coor, eta_list, rs_list, rc_list)\n",
    "\n",
    "coor = np.zeros((3, 3))\n",
    "coor[1] = np.array([1, 0, 0])\n",
    "coor[2] = np.array([0, 1, 0])\n",
    "# print coor\n",
    "with open('./coor/100.xyz', 'r') as fp:\n",
    "    coor = fp.readlines()\n",
    "coor = coor[2: ]\n",
    "coor = np.array([map(float, line.split()[1: 4]) for line in coor])\n",
    "# coor\n",
    "param = {}\n",
    "param['radial'] = {'eta_list':  [0.12, 0.2], 'rs_list': [0.0], 'rc_list': [6.0]}\n",
    "param['angle'] = {'eta_list':  [0.03, 0.09], 'lambda_list':  [-1.0, 1.0],\n",
    "                  'zeta_list': [1.0, 2.0, 4.0], 'rc_list': [6.0]}\n",
    "# param['angle'] = {'eta_list':  [1], 'lambda_list':  [1.0],\n",
    "#                   'zeta_list': [1.0], 'rc_list': [6.0]}\n",
    "dt = DataTransform(param)\n",
    "start_time = time.time()\n",
    "b = dt.transform(coor)\n",
    "elapsed_time = time.time() - start_time\n",
    "print elapsed_time\n",
    "# print coor\n",
    "# print b\n",
    "# dt.infer_feature_size()\n",
    "# dt.angular_symm_func(coor) == angular_symm_func(coor, eta_list, lambda_list, zeta_list, rc_list)\n",
    "# dt.radial_symm_func(coor) == radial_symm_func(coor, eta_list, rs_list, rc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones(((3, 3))).reshape(-1).astype(int)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1077650057774068e-30"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((a-b)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arccos(np.float64(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Blob:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_ = None\n",
    "        self.diff_ = None\n",
    "        return\n",
    "    \n",
    "    def setup(self, shape):\n",
    "        num_args = len(shape)\n",
    "        assert num_args == 1 or num_args == 2, 'the number of arguments should either be 1 or 2'\n",
    "        self.data_ = np.empty(shape, dtype=float)  # of size (natoms#1+natoms#2+...) * feature_size\n",
    "        self.diff_ = np.empty(shape, dtype=float) # of same size of self.coors\n",
    "        self.natom_ = np.empty(0, dtype=int) # array with [natoms#1, natoms#2, ...]\n",
    "        return\n",
    "        \n",
    "    \n",
    "#     def reset(self):\n",
    "#         self.data_ = np.empty((0, feature_size), dtype=float)  # of size (natoms#1+natoms#2+...) * feature_size\n",
    "#         self.diff_ = np.empty((0, feature_size), dtype=float) # of same size of self.coors\n",
    "#         self.natom_ = np.empty(0, dtype=int) # array with [natoms#1, natoms#2, ...]\n",
    "#         return\n",
    "    \n",
    "    def add_data(self, data):\n",
    "        '''add coordination data from cluster, data should be size of natoms * feature_size'''\n",
    "        self.data_ = np.vstack((self.data_, data))\n",
    "        self.natom_ = np.hstack((self.natom_, data.shape[0]))\n",
    "        return \n",
    "    \n",
    "#     def get_data(self):\n",
    "#         return self.data_\n",
    "    \n",
    "#     def get_diff(self):\n",
    "#         return self.diff_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-57597.91745042 -57599.01257849]\n",
      "[-57599.81960047 -57600.2605938 ]\n"
     ]
    }
   ],
   "source": [
    "class DataLayer:\n",
    "    '''TODO: handle the situation when idx_ larger the files_.shape()[0]'''\n",
    "    \n",
    "    def __init__(self, param):\n",
    "        #         self.param_ = copy.deepcopy(param)\n",
    "        self.batch_size_ = param['batch_size']\n",
    "        self.source_ = param['source']\n",
    "        self.transform_param_ = copy.deepcopy(param['transform_param'])\n",
    "        \n",
    "        self.idx_ = 0 # the current file number\n",
    "        self.files_ = []\n",
    "        self.transformer_ = DataTransform(self.transform_param_)\n",
    "        self.feature_size_ = self.transformer_.infer_feature_size()\n",
    "        self.setup()\n",
    "    \n",
    "    def setup(self):\n",
    "        with open(self.source_, 'r') as fp:\n",
    "            self.files_ = fp.readlines()\n",
    "        self.files_ = [line.strip() for line in  self.files_]\n",
    "        \n",
    "    def forward(self):\n",
    "        '''\n",
    "        return (Blob, energy) with energy of size batch_size\n",
    "        '''\n",
    "        features, energies = Blob(), np.empty(0, float)\n",
    "        features.setup((0, self.feature_size_))\n",
    "        for i in range(self.idx_, self.idx_ + self.batch_size_):\n",
    "            with open(self.files_[i], 'r') as fp:\n",
    "                xyz_file = fp.readlines()\n",
    "            energy = float(xyz_file[1].split()[-2])\n",
    "            coor = np.array([map(float, line.split()[1: 4]) for line in xyz_file[2: ]])\n",
    "            feature = self.transformer_.transform(coor)\n",
    "            features.add_data(feature)\n",
    "            energies = np.hstack((energies, energy))\n",
    "        self.idx_ += self.batch_size_\n",
    "        return features, energies\n",
    "    \n",
    "transform_param = {}\n",
    "transform_param['radial'] = {'eta_list':  [0.12, 0.2], 'rs_list': [0.0], 'rc_list': [6.0]}\n",
    "transform_param['angle'] = {'eta_list':  [0.03, 0.09], 'lambda_list':  [-1.0, 1.0],\n",
    "                  'zeta_list': [1.0, 2.0, 4.0], 'rc_list': [6.0]}\n",
    "data_param = {}\n",
    "data_param['source'] = './data.txt'\n",
    "data_param['batch_size'] = 2\n",
    "data_param['transform_param'] = transform_param\n",
    "dl = DataLayer(data_param)\n",
    "f, e= dl.forward()\n",
    "print e\n",
    "f, e= dl.forward()\n",
    "print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a = {1:1}\n",
    "print a.get(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Filler:\n",
    "    \n",
    "    def __init__(self, param):\n",
    "        #         self.param_ = copy.deepcopy(param)\n",
    "        self.type_ = param['type']\n",
    "        return\n",
    "    \n",
    "    def fill(self, shape):\n",
    "        #         shape = container.shape\n",
    "        size =  reduce(lambda count, item: count * item, shape, 1)\n",
    "        weight = Blob()\n",
    "        if self.type_ == 'gaussian':\n",
    "            std = param.get('std', 0.01)\n",
    "            weight.data_ = np.random.randn(size).reshape(shape) * std\n",
    "        elif self.type_ == 'constant':\n",
    "            value = param.get('value', 0.0)\n",
    "            weight.data_ = np.ones(size).reshape(shape) * value\n",
    "        return weight\n",
    "    \n",
    "# param = {'type': 'gaussian', 'std': 0.1}\n",
    "# filler = Filler(param)\n",
    "# w = filler.fill(np.array([20, 30]))\n",
    "# plt.hist(w.reshape((-1)), bins=20)\n",
    "\n",
    "\n",
    "# param = {'type': 'constant'}\n",
    "# filler = Filler(param)\n",
    "# w = filler.fill(np.array([20]))\n",
    "# w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PlInnerProductLayer:\n",
    "    '''inner product layer for nnpl'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class InnerProductLayer:\n",
    "    \n",
    "    def __init__(self, param):\n",
    "        self.output_size_ = param['output_size']\n",
    "        self.weight_filler_param_ = copy.deepcopy(param['weight_filler_param'])\n",
    "        self.bias_filler_param_ = copy.deepcopy(param['bias_filler_param'])\n",
    "        self.input_size_ = None\n",
    "        self.batch_size_ = None\n",
    "        self.weight_ = None\n",
    "        self.bias_ = None\n",
    "        return\n",
    "        \n",
    "    def setup(self, bottoms, tops):\n",
    "        assert len(bottoms) == 1, 'layer InnerProductLayer only support bottoms of length 1'\n",
    "        assert len(tops) == 1, 'layer InnerProductLayer only support tops of length 1'\n",
    "        data = bottoms[0].data_\n",
    "        self.batch_size_, self.input_size_ = data.shape\n",
    "        weight_filler = Filler(self.weight_filler_param_)\n",
    "        self.weight_ = weight_filler.fill((self.input_size_, self.output_size_))\n",
    "        bias_filler = Filler(self.bias_filler_param_)\n",
    "        self.bias_ = bias_filler.fill((self.output_size_, ))\n",
    "        return\n",
    "    \n",
    "    def forward(self, bottoms, tops):\n",
    "        tops[0].data_ = np.dot(bottoms[0].data_, self.weight_.data_) + self.bias_.data_\n",
    "        return\n",
    "        \n",
    "    def backward(self, bottoms, tops):\n",
    "        bottoms[0].diff_ = np.dot(self.weight_.data_, tops[0].diff_.T).T\n",
    "        self.weight_.diff_ = np.dot(bottoms[0].data_.T, tops[0].diff_)\n",
    "        self.bias_.diff_ = np.sum(tops[0].diff_, axis=0)\n",
    "        return\n",
    "\n",
    "layer_register('InnerProduct', InnerProductLayer)\n",
    "# weight_filler_param = {'type': 'gaussian', 'std': 0.1}\n",
    "# bias_filler_param = {'type': 'constant'}\n",
    "# ip_param = {}\n",
    "# ip_param['output_size'] = 20\n",
    "# ip_param['weight_filler_param'] = weight_filler_param\n",
    "# ip_param['bias_filler_param'] = bias_filler_param\n",
    "\n",
    "# bottoms[0] = Blob()\n",
    "# bottoms[0].data_ = np.random.randn(100, 40)\n",
    "# tops[0] = Blob()\n",
    "\n",
    "# ip_layer = InnerProductLayer(ip_param)\n",
    "# ip_layer.setup(bottoms[0], tops[0])\n",
    "# ip_layer.forward(bottoms[0], tops[0])\n",
    "\n",
    "# tops[0].diff_ = np.random.randn(100, 20)\n",
    "# ip_layer.backward(bottoms[0], tops[0])\n",
    "# print bottoms[0].diff_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SigmoidLayer:\n",
    "    def __init__(self, param):\n",
    "        return\n",
    "    \n",
    "    def setup(self, bottoms, tops):\n",
    "        assert len(bottoms) == 1, 'layer SigmoidLayer only support bottoms of length 1'\n",
    "        assert len(tops) == 1, 'layer SigmoidLayer only support tops of length 1'\n",
    "        return\n",
    "    \n",
    "    def forward(self, bottoms, tops):\n",
    "        tops[0].data_ = 1.0 / (1.0 + np.exp(-bottoms[0].data_))\n",
    "        return\n",
    "    \n",
    "    def backward(self, bottoms, tops):\n",
    "        sig_diff = (1.0 - tops[0].data_) * tops[0].data_\n",
    "        bottoms[0].diff_ = tops[0].diff_ * sig_diff\n",
    "        return\n",
    "layer_register('Sigmoid', SigmoidLayer)\n",
    "    \n",
    "# bottoms = [None]\n",
    "# bottoms[0] = Blob()\n",
    "# bottoms[0].data_ = np.random.randn(20, 40)\n",
    "# tops = [None]\n",
    "# tops[0] = Blob()\n",
    "# tops[0].diff_ = np.ones((20, 40))\n",
    "\n",
    "# sig_param = {'name': 'sig'}\n",
    "# sig_layer = SigmoidLayer(sig_param)\n",
    "# sig_layer.forward(bottoms, tops)\n",
    "# plt.plot(bottoms[0].data_.reshape(-1), tops[0].data_.reshape(-1), 'x')\n",
    "# sig_layer.backward(bottoms, tops)\n",
    "# plt.plot(bottoms[0].data_.reshape(-1), bottoms[0].diff_.reshape(-1), 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EuclideanLossLayer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        assert len(bottom) == 2, 'the length of bottom list should be equal to 2'\n",
    "        # bottom[0] should be predicted values, bottom[1] should be ground truth\n",
    "        return\n",
    "    \n",
    "    def setup(self):\n",
    "        return\n",
    "    \n",
    "    def forward(self, bottom, top):\n",
    "        loss = 0.5 * (bottom[0].data_ - bottom[1].data_) ** 2\n",
    "        return \n",
    "    \n",
    "    def backward(self, bottom, top):\n",
    "        bottom[0].diff_ = bottom[0].data_ - bottom[1].data_\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  4  6 10 12]\n",
      "features:\n",
      "[[ 1.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]]\n",
      "labels: \n",
      "[[ 1.  0.]\n",
      " [ 3.  0.]\n",
      " [ 2.  0.]]\n",
      "\n",
      "\n",
      "\n",
      "features:\n",
      "[[ 4.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 1.]]\n",
      "labels: \n",
      "[[ 4.  0.]\n",
      " [ 2.  0.]\n",
      " [ 1.  0.]]\n",
      "\n",
      "\n",
      "\n",
      "features:\n",
      "[[ 3.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 4.]]\n",
      "labels: \n",
      "[[ 3.  0.]\n",
      " [ 2.  0.]\n",
      " [ 4.  0.]]\n",
      "\n",
      "\n",
      "\n",
      "features:\n",
      "[[ 2.]\n",
      " [ 2.]\n",
      " [ 1.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]]\n",
      "labels: \n",
      "[[ 2.  0.]\n",
      " [ 1.  0.]\n",
      " [ 3.  0.]\n",
      " [ 2.  0.]]\n",
      "\n",
      "\n",
      "\n",
      "features:\n",
      "[[ 4.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 1.]]\n",
      "labels: \n",
      "[[ 4.  0.]\n",
      " [ 2.  0.]\n",
      " [ 1.  0.]]\n",
      "\n",
      "\n",
      "\n",
      "features:\n",
      "[[ 3.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 4.]]\n",
      "labels: \n",
      "[[ 3.  0.]\n",
      " [ 2.  0.]\n",
      " [ 4.  0.]]\n",
      "\n",
      "\n",
      "\n",
      "features:\n",
      "[[ 2.]\n",
      " [ 2.]\n",
      " [ 1.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]]\n",
      "labels: \n",
      "[[ 2.  0.]\n",
      " [ 1.  0.]\n",
      " [ 3.  0.]\n",
      " [ 2.  0.]]\n",
      "\n",
      "\n",
      "\n",
      "features:\n",
      "[[ 4.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 1.]]\n",
      "labels: \n",
      "[[ 4.  0.]\n",
      " [ 2.  0.]\n",
      " [ 1.  0.]]\n",
      "\n",
      "\n",
      "\n",
      "features:\n",
      "[[ 3.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 4.]\n",
      " [ 4.]]\n",
      "labels: \n",
      "[[ 3.  0.]\n",
      " [ 2.  0.]\n",
      " [ 4.  0.]]\n",
      "\n",
      "\n",
      "\n",
      "features:\n",
      "[[ 2.]\n",
      " [ 2.]\n",
      " [ 1.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 2.]]\n",
      "labels: \n",
      "[[ 2.  0.]\n",
      " [ 1.  0.]\n",
      " [ 3.  0.]\n",
      " [ 2.  0.]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nnpl.txt_reader import  *\n",
    "from nnpl.layer_factory import *\n",
    "from nnpl.blob import Blob\n",
    "\n",
    "layer_unregister('BehlerTxtData')\n",
    "\n",
    "class BehlerTxtDataLayer:\n",
    "    '''\n",
    "    TODO: support none value in feature file, support shuffle\n",
    "    the actual batch_size is less than the set batch size\n",
    "    Adopted from Behler et. al. PRL 98, 146401 (2007)\n",
    "    '''\n",
    "    def __init__(self, param):\n",
    "        '''{'feature_file': 'path/to/feature/file', 'label_file': 'path/to/label/file', 'batch_size': batch_size, 'num_entries': num_entries}'''\n",
    "        self.blobs_ = None\n",
    "        self.batch_size_ = param['batch_size']\n",
    "        self.feature_reader_ = TxtReader(param['feature_file'])\n",
    "        self.label_reader_ = TxtReader(param['label_file'])\n",
    "        self.label_map_ = None  # map from label to features\n",
    "        self.feature_idx_ = 0 # the current feature index\n",
    "        self.label_idx_ = 0 # the current label index\n",
    "        self.num_feature_entries_ = None # number of features (atoms)\n",
    "        self.num_label_entries_ = None # number of labels (clusters)\n",
    "        self.num_features_ = None\n",
    "        self.num_labels_ = None\n",
    "        return\n",
    "    \n",
    "    def setup(self, bottoms, tops):\n",
    "        '''\n",
    "        tops[0] for feature blob, tops[1] for label blob\n",
    "        '''\n",
    "        assert len(tops) == 2, 'layer TxtDataLayer only support bottoms of length 2'\n",
    "        self.num_features_ = self.feature_reader_.peek().shape[0]\n",
    "        self.num_labels_ = self.label_reader_.peek().shape[0]\n",
    "        tops[0].setup((0, self.num_features_))\n",
    "        tops[1].setup((0, self.num_labels_))\n",
    "        \n",
    "        tops[0].is_update_ = False\n",
    "        tops[1].is_update_ = False\n",
    "        \n",
    "        self.label_map_ = self.label_reader_.fetch_all()[:, 0].astype(int)\n",
    "        for i in range(1, self.label_map_.shape[0]):\n",
    "            self.label_map_[i] += self.label_map_[i-1]\n",
    "        self.num_feature_entries_ = self.label_map_[-1]\n",
    "        self.num_label_entries_ = len(self.label_map_)\n",
    "        return\n",
    "    \n",
    "    def forward(self, bottoms, tops):\n",
    "        # store data in text file is highly inefficent, would better to use database like lmdb or leveldb\n",
    "        # take care of when idx_+batch_size_ larger than num_entries\n",
    "        feature_loops = []\n",
    "        label_loops = []\n",
    "        num_loops = (self.feature_idx_ + self.batch_size_) / self.num_feature_entries_\n",
    "        feature_batch_size = 0  # actual feature batch size\n",
    "        label_batch_size = 0 # actual label batch size\n",
    "        if num_loops == 0:\n",
    "            next_label_idx = np.searchsorted(self.label_map_, self.feature_idx_ + self.batch_size_)\n",
    "            if next_label_idx == 0:\n",
    "                assert False, 'batch size is too small, should be larger than the largest atom number in cluster'\n",
    "            else:\n",
    "                next_feature_idx = self.label_map_[next_label_idx-1]\n",
    "            feature_batch_size = next_feature_idx - self.feature_idx_\n",
    "            label_batch_size = next_label_idx - self.label_idx_\n",
    "            feature_loops.append((self.feature_idx_, next_feature_idx))\n",
    "            label_loops.append((self.label_idx_, next_label_idx))\n",
    "            self.label_idx_ = next_label_idx\n",
    "            self.feature_idx_ = next_feature_idx\n",
    "        else:\n",
    "            feature_loops.append((self.feature_idx_, self.num_feature_entries_))\n",
    "            label_loops.append((self.label_idx_, self.num_label_entries_))\n",
    "            feature_batch_size = self.num_feature_entries_ - self.feature_idx_\n",
    "            label_batch_size = self.num_label_entries_ - self.label_idx_\n",
    "            for i in range(1, num_loops):\n",
    "                feature_batch_size += self.num_feature_entries_\n",
    "                label_batch_size += self.num_label_entries_\n",
    "                feature_loops.append((0, self.num_feature_entries_))\n",
    "                label_loops.append((0, self.num_label_entries_))\n",
    "            res_features = (self.feature_idx_ + self.batch_size_) % self.num_feature_entries_\n",
    "            next_label_idx = np.searchsorted(self.label_map_, res_features)\n",
    "            if next_label_idx == 0:\n",
    "                next_feature_idx = 0\n",
    "            else:\n",
    "                next_feature_idx = self.label_map_[next_label_idx-1]\n",
    "                feature_batch_size += next_feature_idx\n",
    "                label_batch_size += next_label_idx\n",
    "                feature_loops.append((0, next_feature_idx))\n",
    "                label_loops.append((0, next_label_idx))\n",
    "            self.label_idx_ = next_label_idx\n",
    "            self.feature_idx_ = next_feature_idx\n",
    "        \n",
    "        tops[0].setup((feature_batch_size, self.num_features_))\n",
    "        tops[1].setup((label_batch_size, self.num_labels_))\n",
    "        feature_batch_idx = 0\n",
    "        label_batch_idx = 0\n",
    "        for i in range(len(feature_loops)):\n",
    "            tops[0].data_[feature_batch_idx: feature_batch_idx + feature_loops[i][1] - feature_loops[i][0]] = \\\n",
    "                self.feature_reader_.fetch_data(feature_loops[i][0], feature_loops[i][1])\n",
    "            tops[1].data_[label_batch_idx: label_batch_idx + label_loops[i][1] - label_loops[i][0]] = \\\n",
    "                self.label_reader_.fetch_data(label_loops[i][0], label_loops[i][1])\n",
    "            feature_batch_idx = feature_batch_idx + feature_loops[i][1] - feature_loops[i][0]\n",
    "            label_batch_idx = label_batch_idx + label_loops[i][1] - label_loops[i][0]\n",
    "        return\n",
    "    \n",
    "    def backward(self, bottoms, tops):\n",
    "        return\n",
    "\n",
    "layer_register('BehlerTxtData', BehlerTxtDataLayer)\n",
    "# txt_data_param = {'batch_size': 10, 'feature_file': './test/feature.txt', 'label_file': './test/label.txt'}\n",
    "# tdl = BehlerTxtDataLayer(txt_data_param)\n",
    "\n",
    "\n",
    "# fb, lb = Blob(), Blob()\n",
    "# tops = [fb, lb]\n",
    "# bottoms = None\n",
    "\n",
    "# tdl.setup(bottoms, tops)\n",
    "# print tdl.label_map_\n",
    "# for i in range(10):\n",
    "#     tdl.forward(bottoms, tops)\n",
    "#     print 'features:\\n', tops[0].data_\n",
    "#     print 'labels: \\n', tops[1].data_\n",
    "#     print '\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 3, 5, 6])\n",
    "np.searchsorted(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "1.56155145765\n"
     ]
    }
   ],
   "source": [
    "from nnpl.layer_factory import *\n",
    "import numpy as np\n",
    "from nnpl.blob import Blob\n",
    "\n",
    "layer_unregister('BehlerEuclideanLoss')\n",
    "\n",
    "class BehlerEuclideanLossLayer:\n",
    "    '''\n",
    "    Adopted from Behler et. al. PRL 98, 146401 (2007)\n",
    "    bottoms[0] of shape (batch_size * 1), predicted output values\n",
    "    bottoms[1] of shape [num_label_entries * 2], ground truth values\n",
    "    For each line in bottoms[1]: [number of ouput entry corresponding to the same label, label]\n",
    "    assert sum(label[:, 0]) == output.shape[0]\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, param):\n",
    "        self.blobs_ = None\n",
    "        output2label_mat = None\n",
    "        return\n",
    "    \n",
    "    def setup(self, bottoms, tops):\n",
    "        assert len(bottoms) == 2, 'layer BehlerEuclideanLossLayer only support bottoms of length 2'\n",
    "        assert bottoms[0].data_.shape[1] == 1, 'layer BehlerEuclideanLossLayer only support one output for each input'\n",
    "        assert bottoms[1].data_.shape[1] == 2, 'layer BehlerEuclideanLossLayer only support [num_label_entries * 2]'\n",
    "        return\n",
    "    \n",
    "    def forward(self, bottoms, tops):\n",
    "        num_output_to_label = bottoms[1].data_[:, 0].astype(int)\n",
    "        num_labels = bottoms[1].data_.shape[0]\n",
    "        num_outputs = bottoms[0].data_.shape[0]\n",
    "        assert np.sum(num_output_to_label) == num_outputs, 'Error'\n",
    "        ouput2label_map = np.zeros(num_labels+1, dtype=int)\n",
    "        for i in range(1, num_labels+1):\n",
    "            ouput2label_map[i] = ouput2label_map[i-1] + num_output_to_label[i-1]\n",
    "        output2label_mat = np.zeros((num_labels, num_outputs))\n",
    "        print num_outputs\n",
    "        for i in range(num_labels):\n",
    "            output2label_mat[i, ouput2label_map[i]: ouput2label_map[i+1]] = 1\n",
    "        pred_value = np.dot(output2label_mat, bottoms[0].data_)\n",
    "        gt_value =  bottoms[1].data_[:, 1]\n",
    "        loss = 0.5 * np.sum((pred_value - gt_value) ** 2) / num_outputs\n",
    "        print loss\n",
    "        return\n",
    "    \n",
    "    def backward(self, bottoms, tops):\n",
    "        num_output_to_label = bottoms[1].data_[:, 0].astype(int)\n",
    "        num_labels = bottoms[1].data_.shape[0]\n",
    "        num_outputs = bottoms[0].data_.shape[0]\n",
    "        assert np.sum(num_output_to_label) == num_outputs, 'Error'\n",
    "        ouput2label_map = np.zeros(num_labels+1, dtype=int)\n",
    "        for i in range(1, num_labels+1):\n",
    "            ouput2label_map[i] = ouput2label_map[i-1] + num_output_to_label[i-1]\n",
    "        output2label_mat = np.zeros((num_labels, num_outputs))\n",
    "        for i in range(num_labels):\n",
    "            output2label_mat[i, ouput2label_map[i]: ouput2label_map[i+1]] = 1\n",
    "        pred_value = np.dot(output2label_mat, bottoms[0].data_)\n",
    "        gt_value =  bottoms[1].data_[:, 1]\n",
    "        tmp_diff = pred_value - gt_value\n",
    "        bottoms[0].diff_ = np.dot(output2label_mat.T, tmp_diff)\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "layer_register('BehlerEuclideanLoss', BehlerEuclideanLossLayer)\n",
    "\n",
    "# bottoms = [None] * 2\n",
    "# bottoms[0] = Blob()\n",
    "# bottoms[0].data_ = np.random.randn(20, 1)\n",
    "# bottoms[1] = Blob()\n",
    "# bottoms[1].data_ = np.array([[15, 0], [5, 0]])\n",
    "# tops = []\n",
    "\n",
    "# param = None\n",
    "# el_layer = BehlerEuclideanLossLayer(param)\n",
    "# el_layer.forward(bottoms, tops)\n",
    "# el_layer.backward(bottoms, tops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]]\n",
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([2, 3, 1])\n",
    "num_labels = a.shape[0]\n",
    "num_outputs = np.sum(a)\n",
    "b = np.zeros((num_labels, num_outputs))\n",
    "c = np.zeros(num_labels+1, dtype=int)\n",
    "for i in range(1, num_labels+1):\n",
    "    c[i] = c[i-1] + a[i-1]\n",
    "for i in range(num_labels):\n",
    "    b[i, c[i]: c[i+1]] = 1\n",
    "print b\n",
    "print b.T\n",
    "d = np.ones((6, 1))\n",
    "# np.dot(b, d)\n",
    "# print d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7d95d5c6d0>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEACAYAAAC57G0KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBDwAZOr/AWf/I67ayxNfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d95dcbbd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SoftmaxLossLayer:\n",
    "    '''\n",
    "    bottoms[0] of shape (batch_size * num_labels), unnormalized exp prob\n",
    "    bottoms[1] of shape(batch_size * 1), ground truth label for each batch\n",
    "    entries bottom[1] should be in range(0, num_labels)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, param):\n",
    "        self.prob_ = None\n",
    "        return\n",
    "    \n",
    "    def setup(self, bottoms, tops):\n",
    "        assert len(bottoms) == 2, 'layer SoftmaxLossLayer only support bottoms of length 1'\n",
    "        return\n",
    "    \n",
    "    def forward(self, bottoms, tops):\n",
    "        self.prob_ = np.exp(bottoms[0].data_ - np.max(bottoms[0].data_, axis=1, keepdims=True))\n",
    "        self.prob_ /= np.sum(self.prob_, axis=1, keepdims=True)\n",
    "        N = bottoms[0].data_.shape[0]\n",
    "        loss = -np.sum(np.log(self.prob_[np.arange(N), bottoms[1].data_])) / N\n",
    "        print loss\n",
    "        return\n",
    "    \n",
    "    def backward(self, bottoms, tops):\n",
    "        bottoms[0].diff_ = self.prob_.copy()\n",
    "        N = bottoms[0].data_.shape[0]\n",
    "        bottoms[0].diff_[np.arange(N), bottoms[1].data_] -= 1\n",
    "        bottoms[0].diff_ /= N\n",
    "        return\n",
    "\n",
    "layer_register('SoftmaxLoss', SoftmaxLossLayer)\n",
    "\n",
    "bottoms = [None] * 2\n",
    "bottoms[0] = Blob()\n",
    "bottoms[0].data_ = np.random.randn(20, 40)\n",
    "bottoms[1] = Blob()\n",
    "bottoms[1].data_ = np.random.randint(40, size=20)\n",
    "tops = []\n",
    "\n",
    "sm_layer = SoftmaxLossLayer()\n",
    "sm_layer.forward(bottoms, tops)\n",
    "sm_layer.backward(bottoms, tops)\n",
    "plt.plot(bottoms[0].data_[0, :], sm_layer.prob_[0, :], 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NnplException(Exception):\n",
    "    def __init__(self, s):\n",
    "        self.s_ = s\n",
    "        print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net:\n",
    "    '''\n",
    "    Param shoud be of format [{'name': 'layer1', 'bottoms': ['blob1', 'blob2'], 'tops': ['blob1', 'blob2'], 'param': param info}, ...]\n",
    "    \n",
    "    Blob names and layer names are used by Net to bookkeeping the blobs and layers to prevent use undefined blob or \n",
    "    double define blobs\n",
    "    \n",
    "    layers use param to initilize. use bottoms and tops to setup up and initilize for internal weights and bias\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.num_layers_ = len(param)\n",
    "        self.layer_names_ = [layer['name'] for layer in param]\n",
    "        self.layers_ = []\n",
    "        \n",
    "        self.blobs_ = []\n",
    "        self.top_blobs_ = []\n",
    "        self.bottom_blobs_ = []\n",
    "        self.blob_name2idx_ = {}\n",
    "        self.blob_names_ = []\n",
    "        \n",
    "        for param in params:\n",
    "            # initialize layer\n",
    "            layer = layer_creator(param['param'])\n",
    "            self.layers_.append(layer)\n",
    "            bottoms = []\n",
    "            tops = []\n",
    "            \n",
    "            # initialize bottom blobs\n",
    "            for bottom_name in param['bottoms']:\n",
    "                assert bottom_name in self.blob_names, 'blob %s has note defined'%bottom_name\n",
    "                blob_id = self.blob_name2idx_[bottom_name]\n",
    "                bottoms.append(self.blobs_[blob_id])\n",
    "            self.bottom_blobs_.append(bottoms)\n",
    "            \n",
    "            # initialize top blobs\n",
    "            for top_name in param['tops']:\n",
    "                if top_name in self.blob_names:\n",
    "                    if top_name in bottoms:\n",
    "                        # inplace layer, to be implemented\n",
    "                        raise NnplException('blob %s has already been defined. Not support inplace layer yet'%top_name)\n",
    "                    if top_name not in bottoms:\n",
    "                        raise NnplException('blob %s has already been defined.'%top_name)\n",
    "                else:\n",
    "                    # blob top_name has not been defined\n",
    "                    blob_id = len(blob_names_)\n",
    "                    self.blob_name2idx_[top_name] = blob_id\n",
    "                    self.blob_names_.append(top_name)\n",
    "                    top = Blob()\n",
    "                    tops.append(top)\n",
    "                    self.blobs_.append(top)\n",
    "            self.top_blobs_.append(tops)\n",
    "            \n",
    "            # setup layer\n",
    "            layer.setup(bottoms, tops)\n",
    "        return\n",
    "    \n",
    "    def forward():\n",
    "        loss = 0.0\n",
    "        for i in range(self.num_layers_):\n",
    "            layer = self.layers_[i]\n",
    "            layer.forward(self.bottom_blobs_[i], self.top_blobs_[i])\n",
    "#             loss += layer.forward(self.bottom_blobs_[i], self.top_blobs_[i])\n",
    "        return loss\n",
    "    \n",
    "    def backward():\n",
    "        for i in reversed(range(self.num_layers_)):\n",
    "            layer = self.layers_[i]\n",
    "            layer.backward(self.bottom_blobs_[i], self.top_blobs_[i])\n",
    "        return\n",
    "    \n",
    "    def forward_backward():\n",
    "        loss= forward()\n",
    "        backward()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TxtReader:\n",
    "    '''\n",
    "    very inefficent\n",
    "    '''\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path_ = file_path\n",
    "        self.multiple_label_ = None\n",
    "        with open(self.file_path_, 'r') as fp:\n",
    "            line = fp.readline()\n",
    "        if len(line.split(',')) == 1:\n",
    "            self.multiple_label_=False\n",
    "        else:\n",
    "            self.multiple_label_=True\n",
    "        return\n",
    "    \n",
    "    def fetch_data(self, line_num):\n",
    "        with open(self.file_path_, 'r') as fp:\n",
    "            line = fp.readlines()[line_num]\n",
    "        if self.multiple_label_:\n",
    "            result = np.array(map(float, line.split(',')))\n",
    "        else:\n",
    "            result = np.array(float(line.strip()))\n",
    "        return result\n",
    "        \n",
    "    def fetch_data(self, start_line, end_line):\n",
    "        with open(self.file_path_, 'r') as fp:\n",
    "            lines = fp.readlines()[start_line: end_line]\n",
    "        if self.multiple_label_:\n",
    "            result = np.array([map(float, line.split(',')) for line in lines])\n",
    "        else:\n",
    "            result = np.array([float(line.strip()) for line in lines])\n",
    "        return result\n",
    "        return np.array([map(float, line.split(',')) for line in lines])\n",
    "        \n",
    "    def peek(self):\n",
    "        with open(self.file_path_, 'r') as fp:\n",
    "            line = fp.readline()\n",
    "        if self.multiple_label_:\n",
    "            result = np.array(map(float, line.split(',')))\n",
    "        else:\n",
    "            result = np.array(float(line.strip()))\n",
    "        return result\n",
    "    \n",
    "# a = TxtReader('./iris/labels.dat')\n",
    "# print a.multiple_label_\n",
    "# a.fetch_data(0, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_creator = {}\n",
    "def layer_register(layer_name, layer):\n",
    "    assert layer_name not in layer_creator.keys(), 'layer %s has already been defined'%layer_name\n",
    "    layer_creator[layer_name] = layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TxtDataLayer:\n",
    "    '''\n",
    "    TODO: support none value in feature file, support shuffle\n",
    "    data layer for inputs from text format\n",
    "    expecting two txt files each for features and labels\n",
    "    one line for one entry\n",
    "    features should be seperated by commas\n",
    "    only support single label for each entry\n",
    "    '''\n",
    "    def __init__(self, param):\n",
    "        '''{'feature_file': 'path/to/feature/file', 'label_file': 'path/to/label/file', 'batch_size': batch_size, 'num_entries': num_entries}'''\n",
    "        self.batch_size_ = param['batch_size']\n",
    "        self.feature_reader_ = TxtReader(param['feature_file'])\n",
    "        self.label_reader_ = TxtReader(param['label_file'])\n",
    "        \n",
    "        self.idx_ = 0 # the current file number\n",
    "        self.num_entries_ = param['num_entries'] # the number of entries in the feature file\n",
    "        return\n",
    "    \n",
    "    def setup(self, bottoms, tops):\n",
    "        '''\n",
    "        tops[0] for feature blob, tops[1] for label blob\n",
    "        '''\n",
    "        num_features = self.feature_reader_.peek().shape[0]\n",
    "        tops[0].setup((self.batch_size_, num_features))\n",
    "        tops[1].setup((self.batch_size_, ))\n",
    "        return\n",
    "    \n",
    "    def forward(self, bottoms, tops):\n",
    "        # store data in text file is highly inefficent, would better to use database like lmdb or leveldb\n",
    "        # take care of when idx_+batch_size_ larger than num_entries\n",
    "        loops = []\n",
    "        num_loops = (self.idx_ + self.batch_size_) / self.num_entries_\n",
    "        if num_loops == 0:\n",
    "            loops.append((self.idx_, self.idx_+self.batch_size_))\n",
    "        else:\n",
    "            loops.append((self.idx_, self.num_entries_))\n",
    "            for i in range(1, num_loops):\n",
    "                loops.append((0, self.num_entries_))\n",
    "            loops.append((0, (self.idx_ + self.batch_size_) % self.num_entries_))\n",
    "        self.idx_ = (self.idx_ + self.batch_size_) % self.num_entries_\n",
    "        \n",
    "        batch_idx = 0\n",
    "        for loop in loops:\n",
    "            tops[0].data_[batch_idx: batch_idx + loop[1] - loop[0]] = self.feature_reader_.fetch_data(loop[0], loop[1])\n",
    "            tops[1].data_[batch_idx: batch_idx + loop[1] - loop[0]] = self.label_reader_.fetch_data(loop[0], loop[1])\n",
    "            batch_idx = batch_idx + loop[1] - loop[0]\n",
    "        return\n",
    "\n",
    "layer_register('TxtData', TxtDataLayer)\n",
    "# txt_data_param = {'batch_size': 50, 'feature_file': './iris/features.dat', 'label_file': './iris/labels.dat', 'num_entries': 150}\n",
    "# tdl = TxtDataLayer(txt_data_param)\n",
    "\n",
    "# fb, lb = Blob(), Blob()\n",
    "# tops = [fb, lb]\n",
    "# bottoms = None\n",
    "\n",
    "# tdl.setup(bottoms, tops)\n",
    "# tdl.forward(bottoms, tops)\n",
    "# print tops[0].data_[:2]\n",
    "# print tops[1].data_[:2]\n",
    "# tdl.forward(bottoms, tops)\n",
    "# print tops[0].data_[:2]\n",
    "# print tops[1].data_[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bias_filler_param': {'type': 'constant'},\n",
       " 'output_size': 20,\n",
       " 'weight_filler_param': {'std': 0.1, 'type': 'gaussian'}}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_data_param = {}\n",
    "txt_data_param['tops'] = ['features', 'labels']\n",
    "txt_data_param['bottoms'] = []\n",
    "txt_data_param['batch_size'] = 50\n",
    "txt_data_param['feature_file'] = './iris/features.dat'\n",
    "txt_data_param['label_file'] = 'label_file': './iris/labels.dat'\n",
    "txt_data_param['num_entries'] = 150\n",
    "txt_data_param['name'] = 'Data'\n",
    "\n",
    "weight_filler_param1 = {'type': 'gaussian', 'std': 0.1}\n",
    "bias_filler_param1 = {'type': 'constant'}\n",
    "ip_param1 = {}\n",
    "ip_param1['tops'] = ['ip1']\n",
    "ip_param1['bottoms'] = ['features']\n",
    "ip_param1['output_size'] = 20\n",
    "ip_param1['name'] = 'ip1'\n",
    "ip_param1['weight_filler_param1'] = weight_filler_param1\n",
    "ip_param1['bias_filler_param1'] = bias_filler_param1\n",
    "\n",
    "sig_param = {}\n",
    "sig_param['name'] = 'sig'\n",
    "sig_param['bottoms'] = ['ip1']\n",
    "sig_param['tops'] = ['sig']\n",
    "\n",
    "weight_filler_param2 = {'type': 'gaussian', 'std': 0.1}\n",
    "bias_filler_param2 = {'type': 'constant'}\n",
    "ip_param2 = {}\n",
    "ip_param2['tops'] = ['ip2']\n",
    "ip_param2['bottoms'] = ['sig']\n",
    "ip_param2['output_size'] = 3\n",
    "ip_param2['name'] = 'ip2'\n",
    "ip_param2['weight_filler_param2'] = weight_filler_param2\n",
    "ip_param2['bias_filler_param2'] = bias_filler_param2\n",
    "\n",
    "sm_param = {}\n",
    "sm_param['bottoms'] = ['ip2', 'labels']\n",
    "sm_param['tops'] = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-6abc303209ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    return\n",
    "\n",
    "loss = 0\n",
    "loss += test()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "[(22, 150), (0, 150), (0, 72)]\n"
     ]
    }
   ],
   "source": [
    "idx_ = 22\n",
    "batch_size_ = 350\n",
    "num_entries_ = 150\n",
    "loops = []\n",
    "num_loops = (idx_ + batch_size_) / num_entries_\n",
    "if num_loops == 0:\n",
    "    loops.append((idx_, idx_+batch_size_))\n",
    "else:\n",
    "    loops.append((idx_, num_entries_))\n",
    "    for i in range(1, num_loops):\n",
    "        loops.append((0, num_entries_))\n",
    "    loops.append((0, (idx_ + batch_size_) % num_entries_))\n",
    "idx_ = (idx_ + batch_size_) % num_entries_\n",
    "print idx_\n",
    "print loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "150-22+150+72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    '''\n",
    "    expected param:\n",
    "    {'net': [layer1_param, layer2_param], 'type': 'opt type', 'opt_param': {}}\n",
    "    '''\n",
    "    \n",
    "    def __init(param):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 2, 3), dtype=float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Test1:\n",
    "    def __init__(self):\n",
    "        print 'Test1'\n",
    "        return\n",
    "    \n",
    "class Test2:\n",
    "    def __init__(self):\n",
    "        print 'Test2'\n",
    "        return\n",
    "\n",
    "# creator = {'Test1': Test1, 'Test2': Test2}\n",
    "# t1 = creator['Test1']()\n",
    "a = [1, 2]\n",
    "# assert 4 in a, 'hello'\n",
    "np.empty((0, 2, 3), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# a soft cutoff function with respect of inter-atom length r\n",
    "# from Behler et. al. PRL 98, 146401 (2007)\n",
    "def soft_cutoff(r, rc):\n",
    "    if r > rc:\n",
    "        return 0\n",
    "    if r <= rc:\n",
    "        return 0.5 * (np.cos(np.pi * r / rc) + 1)\n",
    "\n",
    "# distance between two atoms\n",
    "def dist(a1, a2):\n",
    "    return np.sqrt(np.sum((a1-a2)**2))\n",
    "\n",
    "# angle between center atom and two reference atoms\n",
    "def angle (ca, ra1, ra2):\n",
    "    b1 = ra1 - ca\n",
    "    b2 = ra2 - ca\n",
    "    return np.arccos(np.dot(b1, b2) / (dist(ca, ra1) * dist(ca, ra2)))\n",
    "    \n",
    "# Radial symmetry functions\n",
    "def radial_symm_func(coor, eta_list, rs_list, rc_list):\n",
    "    args_list = make_grid(eta_list, rs_list, rc_list)\n",
    "    num_atoms = coor.shape[0]\n",
    "    result = np.zeros((num_atoms, len(args_list)))\n",
    "    for center_atom in range(num_atoms):\n",
    "        for args_num in range(len(args_list)):\n",
    "            eta, rs, rc = args_list[args_num]\n",
    "            for ref_atom in range(num_atoms):\n",
    "                # ra stands for reference atom\n",
    "                if ref_atom == center_atom:\n",
    "                    continue\n",
    "                dist_ref_center = dist(coor[ref_atom], coor[center_atom])\n",
    "                # symmetry function between center atom and reference atom\n",
    "                gcr = soft_cutoff(dist_ref_center, rc) * np.exp(-eta*(dist_ref_center-rs)**2)\n",
    "                result[center_atom, args_num]  += gcr\n",
    "    return result\n",
    "    \n",
    "# Angular symmetry functions\n",
    "def angular_symm_func(coor, eta_list, lambda_list, zeta_list, rc_list):\n",
    "    args_list = make_grid(eta_list, lambda_list, zeta_list, rc_list)\n",
    "    num_atoms = coor.shape[0]\n",
    "    result = np.zeros((num_atoms, len(args_list)))\n",
    "    for center_atom in range(num_atoms):\n",
    "        for args_num in range(len(args_list)):\n",
    "            eta, lambda_, zeta, rc = args_list[args_num]\n",
    "            for ref_atom_1 in range(num_atoms):\n",
    "                for ref_atom_2 in range(num_atoms):\n",
    "                    if ref_atom_1 == center_atom or ref_atom_2 == center_atom or ref_atom_1 == ref_atom_2:\n",
    "                        continue\n",
    "                    dist_ref1_center = dist(coor[ref_atom_1], coor[center_atom])\n",
    "                    dist_ref2_center = dist(coor[ref_atom_2], coor[center_atom])\n",
    "                    dist_ref1_ref2 = dist(coor[ref_atom_1], coor[ref_atom_2])\n",
    "                    # symmetry function \n",
    "                    gcr = soft_cutoff(dist_ref1_center, rc) * soft_cutoff(dist_ref2_center, rc) * soft_cutoff(dist_ref1_ref2, rc)\n",
    "                    gcr *= 2 ** (1 - zeta)\n",
    "                    gcr *= (1 + lambda_ * np.cos(angle(center_atom, ref_atom_1, ref_atom_2))) ** zeta\n",
    "                    gcr *= np.exp(-eta*(dist_ref1_center**2+dist_ref2_center**2+dist_ref1_ref2**2))\n",
    "                    result[center_atom, args_num]  += gcr\n",
    "    return result\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eta_list = [0.03, 0.09]\n",
    "lambda_list = [-1.0, 1.0]\n",
    "zeta_list = [1.0, 2.0, 4.0]\n",
    "rc_list = [6.0]\n",
    "# angular_symm_func(coor, eta_list, lambda_list, zeta_list, rc_list)\n",
    "\n",
    "eta_list = [0.12, 0.2]\n",
    "rs_list = [0.0]\n",
    "rc_list = [6.0]\n",
    "# radial_symm_func(coor, eta_list, rs_list, rc_list)\n",
    "\n",
    "\n",
    "param = {}\n",
    "param['radial'] = {'eta_list':  [0.12, 0.2], 'rs_list': [0.0], 'rc_list': [6.0]}\n",
    "param['angle'] = {'eta_list':  [0.03, 0.09], 'lambda_list':  [-1.0, 1.0],\n",
    "                  'zeta_list': [1.0, 2.0, 4.0], 'rc_list': [6.0]}\n",
    "dt = DataTransform(param)\n",
    "dt.transform(coor)\n",
    "# dt.angular_symm_func(coor) == angular_symm_func(coor, eta_list, lambda_list, zeta_list, rc_list)\n",
    "# dt.radial_symm_func(coor) == radial_symm_func(coor, eta_list, rs_list, rc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  2.],\n",
       "       [ 2.,  2.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones((2, 2))\n",
    "b = np.ones((2, 2)) * 2\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
